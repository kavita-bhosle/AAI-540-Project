{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643d7922-c5db-4454-bae4-f126077fc56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Role    : arn:aws:iam::910617065862:role/service-role/AmazonSageMaker-ExecutionRole-20260217T114815\n",
      "Bucket  : sagemaker-us-east-1-910617065862\n",
      "Region  : us-east-1\n",
      "==============================================================\n",
      "  Liver Disease ML Pipeline â€“ AWS SageMaker\n",
      "==============================================================\n",
      "Preprocessing script saved â†’ preprocessing.py\n",
      "Script saved â†’ xgb_train.py\n",
      "Script saved â†’ rf_train.py\n",
      "Script saved â†’ evaluate.py\n",
      "CI/CD files created.\n",
      "Raw data uploaded â†’ s3://sagemaker-us-east-1-910617065862/liver-disease/data/raw/Liver_Patient_Dataset__LPD__train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268/1404510094.py:211: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  df[\"event_time\"] = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature group 'liver-disease-features' already exists (status: Created). Skipping creation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 10864 to 16296\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 21728 to 27158\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 0 to 5432\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 16296 to 21728\n",
      "INFO:sagemaker.feature_store.feature_group:Started ingesting index 5432 to 10864\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 10864 to 16296\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 21728 to 27158\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 16296 to 21728\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 5432 to 10864\n",
      "INFO:sagemaker.feature_store.feature_group:Successfully ingested row 0 to 5432\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 27158 records into Feature Store.\n",
      "Preprocessing script saved â†’ preprocessing.py\n",
      "Old pipeline deleted â€” recreating with updated definition â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline 'LiverDiseasePipeline' created with built-in XGBoost algorithm.\n",
      "Pipeline execution started:\n",
      "  arn:aws:sagemaker:us-east-1:910617065862:pipeline/LiverDiseasePipeline/execution/tcwyq3lb9ad2\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  â¬œ  LiverDiseasePreprocessing              [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  ğŸ”„  LiverDiseasePreprocessing              [Executing]\n",
      "      â””â”€â–º â¬œ  TrainXGBoost                           [NotStarted]\n",
      "      â””â”€â–º â¬œ  TrainRandomForest                      [NotStarted]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  TrainXGBoost                           [Executing]\n",
      "      â””â”€â–º ğŸ”„  TrainRandomForest                      [Executing]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  TrainXGBoost                           [Executing]\n",
      "      â””â”€â–º ğŸ”„  TrainRandomForest                      [Executing]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  TrainXGBoost                           [Executing]\n",
      "      â””â”€â–º ğŸ”„  TrainRandomForest                      [Executing]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  TrainXGBoost                           [Executing]\n",
      "      â””â”€â–º ğŸ”„  TrainRandomForest                      [Executing]\n",
      "      â””â”€â–º â¬œ  EvaluateModels                         [NotStarted]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º ğŸ”„  EvaluateModels                         [Executing]\n",
      "      â””â”€â–º â¬œ  CheckAccuracy                          [NotStarted]\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : ğŸ”„  Executing\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Polling again in 30s â€¦\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  CI/CD PIPELINE DAG\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  âœ…  LiverDiseasePreprocessing              [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainXGBoost                           [Succeeded]\n",
      "      â””â”€â–º âœ…  TrainRandomForest                      [Succeeded]\n",
      "      â””â”€â–º âœ…  EvaluateModels                         [Succeeded]\n",
      "      â””â”€â–º âœ…  CheckAccuracy                          [Succeeded]\n",
      "           â†³  Condition outcome: TRUE\n",
      "      â””â”€â–º â¬œ  RegisterLiverDiseaseModel              [NotStarted]\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "  Overall Status : âœ…  Succeeded\n",
      "\n",
      "  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  All steps completed. Model registered in Model Registry.\n",
      "  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  EXTRACTING PIPELINE ARTIFACTS\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  Training job   : pipelines-tcwyq3lb9ad2-TrainXGBoost-nZ4InXiLSo\n",
      "  Model data S3  : s3://sagemaker-us-east-1-910617065862/liver-disease/output/xgboost/pipelines-tcwyq3lb9ad2-TrainXGBoost-nZ4InXiLSo/output/model.tar.gz\n",
      "  Model Package  : v5 [PendingManualApproval]\n",
      "  Package ARN    : arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/5\n",
      "  Test data S3   : s3://sagemaker-us-east-1-910617065862/LiverDiseasePipeline/tcwyq3lb9ad2/LiverDiseasePreprocessing/output/test\n",
      "  Baseline S3    : s3://sagemaker-us-east-1-910617065862/LiverDiseasePipeline/tcwyq3lb9ad2/LiverDiseasePreprocessing/output/baseline/baseline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2026-02-20-15-49-23-464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  â”€â”€ Evaluation Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  xgboost         acc=1.0  auc=1.0  f1=1.0\n",
      "  random_forest   acc=0.9991  auc=1.0  f1=0.9994\n",
      "  Best model     : xgboost\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "âœ… Model approved: arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name liver-disease-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš   Deployment skipped: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: Cannot create already existing endpoint configuration \"arn:aws:sagemaker:us-east-1:910617065862:endpoint-config/liver-disease-endpoint\".\n",
      "âš   Could not create CloudWatch dashboard (IAM permission missing: cloudwatch:PutDashboard)\n",
      "   Error: An error occurred (AccessDenied) when calling the PutDashboard operation: User: arn:aws:sts::910617065862:assumed-role/AmazonSageMaker-ExecutionRole-20260217T114815/SageMaker is not authorized to perform: cloudwatch:PutDashboard on resource: arn:aws:cloudwatch::910617065862:dashboard/LiverDisease-MLOps-Dashboard because no identity-based policy allows the cloudwatch:PutDashboard action\n",
      "\n",
      "   â”€â”€ To create the dashboard manually â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   1. Open AWS Console â†’ CloudWatch â†’ Dashboards â†’ Create dashboard\n",
      "   2. Name it: LiverDisease-MLOps-Dashboard\n",
      "   3. Add widgets for these metrics:\n",
      "      â€¢ AWS/SageMaker â†’ EndpointName=liver-disease-endpoint â†’ Invocations, ModelLatency\n",
      "      â€¢ AWS/SageMaker â†’ EndpointName=liver-disease-endpoint â†’ CPUUtilization, MemoryUtilization\n",
      "      â€¢ AWS/SageMaker â†’ EndpointName=liver-disease-endpoint â†’ Invocation4XXErrors, Invocation5XXErrors\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Alarm set â†’ LiverDisease-HighLatency\n",
      "  Alarm set â†’ LiverDisease-5xxErrors\n",
      "  Alarm set â†’ LiverDisease-HighCPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Features-only test data â†’ s3://sagemaker-us-east-1-910617065862/liver-disease/batch-input/test_features.csv  (5432 rows Ã— 13 cols)\n",
      "âš   Batch transform skipped: An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-east-1:910617065862:model/liver-disease-xgboost\".\n",
      "âš   Monitoring skipped: endpoint or baseline not available.\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  MODEL MONITORING REPORT\n",
      "  Generated: 2026-02-20 15:49:25 UTC\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”€â”€ DATA QUALITY MONITOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  âš   No violation report found yet (monitor may not have run).\n",
      "\n",
      "â”€â”€ MODEL QUALITY MONITOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  âš   No model quality statistics found yet.\n",
      "\n",
      "â”€â”€ ENDPOINT TRAFFIC (last 1 hour via CloudWatch) â”€â”€â”€â”€\n",
      "  Invocations (1h)    : N/A\n",
      "  Avg Model Latency   : N/A\n",
      "  5xx Errors          : 0\n",
      "  Avg CPU Utilisation : N/A\n",
      "\n",
      "â”€â”€ MODEL REGISTRY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Version    Status                    Created                   ARN\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  5          âœ… Approved               2026-02-20 15:49          arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/5\n",
      "  4          âœ… Approved               2026-02-20 12:46          arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/4\n",
      "  3          âœ… Approved               2026-02-20 12:08          arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/3\n",
      "  2          â³ PendingManualApproval  2026-02-20 11:38          arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/2\n",
      "  1          â³ PendingManualApproval  2026-02-18 09:23          arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/1\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  END OF MONITORING REPORT\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "\n",
      "==============================================================\n",
      "  âœ… PIPELINE COMPLETE â€” ARTIFACT SUMMARY\n",
      "==============================================================\n",
      "  Endpoint        : liver-disease-endpoint\n",
      "  Model data S3   : s3://sagemaker-us-east-1-910617065862/liver-disease/output/xgboost/pipelines-tcwyq3lb9ad2-TrainXGBoost-nZ4InXiLSo/output/model.tar.gz\n",
      "  Model Package   : arn:aws:sagemaker:us-east-1:910617065862:model-package/LiverDiseaseModelGroup/5\n",
      "  Baseline S3     : s3://sagemaker-us-east-1-910617065862/LiverDiseasePipeline/tcwyq3lb9ad2/LiverDiseasePreprocessing/output/baseline/baseline.csv\n",
      "  XGBoost AUC     : 1.0\n",
      "  XGBoost Acc     : 1.0\n",
      "==============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268/1404510094.py:1747: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"  Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
      "/tmp/ipykernel_268/1404510094.py:1866: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now   = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "  AWS SageMaker - Liver Disease Prediction Pipeline\n",
    "  Assignment: End-to-End ML with SageMaker Components\n",
    "  Dataset: https://www.kaggle.com/datasets/abhi8923shriv/liver-disease-patient-dataset\n",
    "============================================================\n",
    "\n",
    "Components Covered:\n",
    "  1. Data Ingestion & Preprocessing\n",
    "  2. SageMaker Feature Store\n",
    "  3. SageMaker Pipelines (Data Pipeline)\n",
    "  4. Model Training & Comparison (XGBoost vs Random Forest)\n",
    "  5. CI/CD with SageMaker Pipelines + CodePipeline\n",
    "  6. Model Deployment (Real-Time Endpoint)\n",
    "  7. Model Monitoring (Data Quality + Model Quality)\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0.  IMPORTS & CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, time, json, boto3, sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition, FeatureTypeEnum\n",
    ")\n",
    "\n",
    "# SageMaker Pipelines\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, TrainingStep, TransformStep\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, ParameterFloat, ParameterString\n",
    ")\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Processing & Training\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    ")\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.xgboost.estimator import XGBoost  # kept for type hints only\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tuner import (\n",
    "    HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    ")\n",
    "\n",
    "# Model Registry & Deployment\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig, DefaultModelMonitor,\n",
    "    CronExpressionGenerator, ModelQualityMonitor\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# â”€â”€ AWS / SageMaker Session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "boto_session   = boto3.Session(region_name=\"us-east-1\")\n",
    "sm_client      = boto_session.client(\"sagemaker\")\n",
    "s3_client      = boto_session.client(\"s3\")\n",
    "\n",
    "sagemaker_session = Session(boto_session=boto_session)\n",
    "role              = get_execution_role()\n",
    "region            = sagemaker_session.boto_region_name\n",
    "\n",
    "BUCKET      = sagemaker_session.default_bucket()\n",
    "PREFIX      = \"liver-disease\"\n",
    "DATA_PREFIX = f\"s3://{BUCKET}/{PREFIX}/data\"\n",
    "OUTPUT_PREFIX = f\"s3://{BUCKET}/{PREFIX}/output\"\n",
    "\n",
    "print(f\"Role    : {role}\")\n",
    "print(f\"Bucket  : {BUCKET}\")\n",
    "print(f\"Region  : {region}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1.  DATA INGESTION â€“ Upload raw CSV to S3\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def upload_raw_data(local_csv_path: str = \"Liver_Patient_Dataset__LPD__train.csv\"):\n",
    "    \"\"\"\n",
    "    Download dataset:\n",
    "      Liver_Patient_Dataset__LPD__train.csv\n",
    "    \"\"\"\n",
    "    s3_uri = sagemaker_session.upload_data(\n",
    "        path=local_csv_path,\n",
    "        bucket=BUCKET,\n",
    "        key_prefix=f\"{PREFIX}/data/raw\"\n",
    "    )\n",
    "    print(f\"Raw data uploaded â†’ {s3_uri}\")\n",
    "    return s3_uri\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2.  PREPROCESSING SCRIPT  (saved to disk, run via Pipeline)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PREPROCESSING_SCRIPT = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\",      type=str, default=\"/opt/ml/processing/input\")\n",
    "    parser.add_argument(\"--train-output\",    type=str, default=\"/opt/ml/processing/train\")\n",
    "    parser.add_argument(\"--test-output\",     type=str, default=\"/opt/ml/processing/test\")\n",
    "    parser.add_argument(\"--baseline-output\", type=str, default=\"/opt/ml/processing/baseline\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # â”€â”€ Load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    csv_file = os.path.join(args.input_data, \"Liver_Patient_Dataset__LPD__train.csv\")\n",
    "    df = pd.read_csv(csv_file, encoding=\"latin-1\")\n",
    "    print(f\"Loaded {len(df)} rows, {df.shape[1]} cols\")\n",
    "\n",
    "    # â”€â”€ Strip invisible chars from column names (\\\\xa0 prefix) â”€â”€\n",
    "    df.columns = df.columns.str.strip().str.replace(\"\\\\xa0\", \"\", regex=False)\n",
    "\n",
    "    # â”€â”€ Rename columns to clean short names â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df.rename(columns={\n",
    "        \"Age of the patient\":                  \"Age\",\n",
    "        \"Gender of the patient\":               \"Gender\",\n",
    "        \"Total Bilirubin\":                     \"Total_Bilirubin\",\n",
    "        \"Direct Bilirubin\":                    \"Direct_Bilirubin\",\n",
    "        \"Alkphos Alkaline Phosphotase\":        \"Alkaline_Phosphotase\",\n",
    "        \"Sgpt Alamine Aminotransferase\":       \"Alamine_Aminotransferase\",\n",
    "        \"Sgot Aspartate Aminotransferase\":     \"Aspartate_Aminotransferase\",\n",
    "        \"Total Protiens\":                      \"Total_Proteins\",\n",
    "        \"ALB Albumin\":                         \"Albumin\",\n",
    "        \"A/G Ratio Albumin and Globulin Ratio\": \"AG_Ratio\",\n",
    "        \"Result\":                              \"Target\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # â”€â”€ Clean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df.dropna(inplace=True)\n",
    "    df[\"Gender\"] = (df[\"Gender\"] == \"Male\").astype(int)\n",
    "    # Target: 1 = liver disease, 0 = no disease  (original: 1=disease, 2=no disease)\n",
    "    df[\"Target\"] = (df[\"Target\"] == 1).astype(int)\n",
    "\n",
    "    # â”€â”€ Feature Engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df[\"Bilirubin_ratio\"] = df[\"Direct_Bilirubin\"] / (df[\"Total_Bilirubin\"] + 1e-6)\n",
    "    df[\"Enzyme_ratio\"]    = (df[\"Alamine_Aminotransferase\"] /\n",
    "                             (df[\"Aspartate_Aminotransferase\"] + 1e-6))\n",
    "    df[\"Age_group\"]       = pd.cut(df[\"Age\"],\n",
    "                                   bins=[0, 30, 50, 70, 120],\n",
    "                                   labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "    print(f\"Class distribution: {df['Target'].value_counts().to_dict()}\")\n",
    "\n",
    "    # â”€â”€ Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c != \"Target\"]\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"Target\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_sc = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_cols)\n",
    "    X_test_sc  = pd.DataFrame(scaler.transform(X_test),      columns=feature_cols)\n",
    "\n",
    "    # â”€â”€ Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    os.makedirs(args.train_output,    exist_ok=True)\n",
    "    os.makedirs(args.test_output,     exist_ok=True)\n",
    "    os.makedirs(args.baseline_output, exist_ok=True)\n",
    "\n",
    "    train_df = pd.concat([y_train.reset_index(drop=True),\n",
    "                          X_train_sc.reset_index(drop=True)], axis=1)\n",
    "    test_df  = pd.concat([y_test.reset_index(drop=True),\n",
    "                          X_test_sc.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    train_df.to_csv(os.path.join(args.train_output,    \"train.csv\"),    index=False, header=False)\n",
    "    test_df.to_csv(os.path.join(args.test_output,      \"test.csv\"),     index=False, header=False)\n",
    "    test_df.to_csv(os.path.join(args.baseline_output,  \"baseline.csv\"), index=False)   # with header for Monitor\n",
    "\n",
    "    print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "    print(\"Preprocessing complete.\")\n",
    "\"\"\"\n",
    "\n",
    "def save_preprocessing_script(path=\"preprocessing.py\"):\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(PREPROCESSING_SCRIPT)\n",
    "    print(f\"Preprocessing script saved â†’ {path}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3.  SAGEMAKER FEATURE STORE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FEATURE_GROUP_NAME = \"liver-disease-features\"\n",
    "\n",
    "def create_feature_group(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Ingest features into SageMaker Feature Store.\n",
    "    Requires two mandatory columns: record_id (String) and event_time (String).\n",
    "    \"\"\"\n",
    "    # Add required FS columns\n",
    "    df = df.copy()\n",
    "    df[\"record_id\"]  = df.index.astype(str)\n",
    "    df[\"event_time\"] = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    df[\"Gender\"]     = df[\"Gender\"].astype(str)   # FS needs consistent types\n",
    "\n",
    "    # â”€â”€ Feature Definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    feature_defs = []\n",
    "    type_map = {\n",
    "        \"object\":  FeatureTypeEnum.STRING,\n",
    "        \"float64\": FeatureTypeEnum.FRACTIONAL,\n",
    "        \"int64\":   FeatureTypeEnum.INTEGRAL,\n",
    "    }\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        fs_type = type_map.get(dtype, FeatureTypeEnum.STRING)\n",
    "        feature_defs.append(FeatureDefinition(feature_name=col, feature_type=fs_type))\n",
    "\n",
    "    # â”€â”€ Create Feature Group â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    fg = FeatureGroup(\n",
    "        name=FEATURE_GROUP_NAME,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        feature_definitions=feature_defs\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Check if feature group already exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    try:\n",
    "        existing = sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP_NAME)\n",
    "        existing_status = existing[\"FeatureGroupStatus\"]\n",
    "        print(f\"Feature group '{FEATURE_GROUP_NAME}' already exists (status: {existing_status}). Skipping creation.\")\n",
    "    except sm_client.exceptions.ResourceNotFound:\n",
    "        # Does not exist â€” create it\n",
    "        fg.create(\n",
    "            s3_uri=f\"s3://{BUCKET}/{PREFIX}/feature-store\",\n",
    "            record_identifier_name=\"record_id\",\n",
    "            event_time_feature_name=\"event_time\",\n",
    "            role_arn=role,\n",
    "            enable_online_store=True,\n",
    "            description=\"Liver disease patient features\"\n",
    "        )\n",
    "        print(f\"Feature group '{FEATURE_GROUP_NAME}' creation initiated â€¦\")\n",
    "\n",
    "        # Wait until Active\n",
    "        status = \"\"\n",
    "        while status != \"Created\":\n",
    "            status = sm_client.describe_feature_group(\n",
    "                FeatureGroupName=FEATURE_GROUP_NAME\n",
    "            )[\"FeatureGroupStatus\"]\n",
    "            print(f\"  Status: {status}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    # â”€â”€ Ingest Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    fg.ingest(data_frame=df, max_workers=5, wait=True)\n",
    "    print(f\"Ingested {len(df)} records into Feature Store.\")\n",
    "    return fg\n",
    "\n",
    "\n",
    "def query_feature_store():\n",
    "    \"\"\"Run an Athena query against the offline Feature Store.\"\"\"\n",
    "    fg_desc = sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP_NAME)\n",
    "    table   = fg_desc[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "    db      = fg_desc[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Database\"]\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT age, gender, total_bilirubin, dataset\n",
    "        FROM \"{db}\".\"{table}\"\n",
    "        WHERE dataset = '1'\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "    from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "    fg = FeatureGroup(name=FEATURE_GROUP_NAME, sagemaker_session=sagemaker_session)\n",
    "    result = fg.athena_query()\n",
    "    result.run(query_string=query,\n",
    "               output_location=f\"s3://{BUCKET}/{PREFIX}/athena-results/\")\n",
    "    result.wait()\n",
    "    return result.as_dataframe()\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4.  SAGEMAKER PIPELINE  (Data + Training + Evaluation)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Pipeline Parameters (configurable at run-time)\n",
    "pipeline_params = dict(\n",
    "    processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1),\n",
    "    training_instance_type    = ParameterString(name=\"TrainingInstanceType\",     default_value=\"ml.m5.xlarge\"),\n",
    "    model_approval_status     = ParameterString(name=\"ModelApprovalStatus\",      default_value=\"PendingManualApproval\"),\n",
    "    accuracy_threshold        = ParameterFloat(name=\"AccuracyThreshold\",         default_value=0.75),\n",
    ")\n",
    "\n",
    "\n",
    "def build_pipeline() -> Pipeline:\n",
    "    \"\"\"\n",
    "    Full SageMaker Pipeline:\n",
    "      Step 1 â€“ Preprocessing (SKLearn Processor)\n",
    "      Step 2 â€“ XGBoost Training  â† built-in algorithm (no custom script)\n",
    "      Step 3 â€“ RF Training       (SKLearn Estimator)\n",
    "      Step 4 â€“ Evaluation\n",
    "      Step 5 â€“ Conditional Registration (if accuracy â‰¥ threshold)\n",
    "    \"\"\"\n",
    "    save_preprocessing_script()   # preprocessing.py still needed for Step 1\n",
    "\n",
    "    # â”€â”€ Step 1 : Preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=\"1.0-1\",\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        instance_count=pipeline_params[\"processing_instance_count\"],\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    step_process = ProcessingStep(\n",
    "        name=\"LiverDiseasePreprocessing\",\n",
    "        processor=sklearn_processor,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=f\"s3://{BUCKET}/{PREFIX}/data/raw\",\n",
    "                destination=\"/opt/ml/processing/input\"\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\",    source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(output_name=\"test\",     source=\"/opt/ml/processing/test\"),\n",
    "            ProcessingOutput(output_name=\"baseline\", source=\"/opt/ml/processing/baseline\"),\n",
    "        ],\n",
    "        code=\"preprocessing.py\"\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Step 2 : XGBoost Training (built-in algorithm â€” no custom script) â”€â”€\n",
    "    # Using the SageMaker built-in XGBoost image avoids all entry_point/module\n",
    "    # issues. The container reads CSV directly; hyperparameters are passed as-is.\n",
    "    xgb_image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",\n",
    "        region=region,\n",
    "        version=\"1.5-1\"\n",
    "    )\n",
    "\n",
    "    xgb_estimator = Estimator(\n",
    "        image_uri=xgb_image_uri,\n",
    "        instance_type=pipeline_params[\"training_instance_type\"],\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        output_path=f\"{OUTPUT_PREFIX}/xgboost\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters={\n",
    "            \"max_depth\":   5,\n",
    "            \"eta\":         0.2,\n",
    "            \"subsample\":   0.8,\n",
    "            \"num_round\":   100,\n",
    "            \"objective\":   \"binary:logistic\",\n",
    "            \"eval_metric\": \"auc\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    step_train_xgb = TrainingStep(\n",
    "        name=\"TrainXGBoost\",\n",
    "        estimator=xgb_estimator,\n",
    "        inputs={\n",
    "            # Built-in XGBoost reads 'train' and 'validation' channel names\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig\n",
    "                        .Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig\n",
    "                        .Outputs[\"test\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "        },\n",
    "        depends_on=[step_process]\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Step 3 : Random Forest Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    rf_estimator = SKLearn(\n",
    "        entry_point=\"rf_train.py\",        # created below\n",
    "        framework_version=\"1.0-1\",\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        output_path=f\"{OUTPUT_PREFIX}/random-forest\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters={\n",
    "            \"n_estimators\": 200,\n",
    "            \"max_depth\":    10,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    step_train_rf = TrainingStep(\n",
    "        name=\"TrainRandomForest\",\n",
    "        estimator=rf_estimator,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig\n",
    "                        .Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "        },\n",
    "        depends_on=[step_process]\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Step 4 : Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    eval_processor = SKLearnProcessor(\n",
    "        framework_version=\"1.0-1\",\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"EvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\"\n",
    "    )\n",
    "\n",
    "    step_eval = ProcessingStep(\n",
    "        name=\"EvaluateModels\",\n",
    "        processor=eval_processor,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_train_xgb.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model/xgboost\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_train_rf.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model/rf\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_process.properties.ProcessingOutputConfig\n",
    "                       .Outputs[\"test\"].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\"\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")\n",
    "        ],\n",
    "        code=\"evaluate.py\",\n",
    "        property_files=[evaluation_report]\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Step 5 : Conditional Model Registration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Register only if accuracy â‰¥ threshold\n",
    "    step_register = RegisterModel(\n",
    "        name=\"RegisterLiverDiseaseModel\",\n",
    "        estimator=xgb_estimator,\n",
    "        model_data=step_train_xgb.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=\"LiverDiseaseModelGroup\",\n",
    "        approval_status=pipeline_params[\"model_approval_status\"],\n",
    "        description=\"XGBoost liver disease model registered via pipeline\"\n",
    "    )\n",
    "\n",
    "    condition_acc = ConditionGreaterThanOrEqualTo(\n",
    "        left=JsonGet(\n",
    "            step_name=step_eval.name,\n",
    "            property_file=evaluation_report,\n",
    "            json_path=\"xgboost.accuracy\"\n",
    "        ),\n",
    "        right=pipeline_params[\"accuracy_threshold\"]\n",
    "    )\n",
    "\n",
    "    step_cond = ConditionStep(\n",
    "        name=\"CheckAccuracy\",\n",
    "        conditions=[condition_acc],\n",
    "        if_steps=[step_register],\n",
    "        else_steps=[]\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Assemble Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    pipeline = Pipeline(\n",
    "        name=\"LiverDiseasePipeline\",\n",
    "        parameters=list(pipeline_params.values()),\n",
    "        steps=[step_process, step_train_xgb, step_train_rf, step_eval, step_cond],\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # Force-delete the old pipeline so the cached definition is fully replaced.\n",
    "    # upsert() alone does not guarantee the execution uses the new step definitions.\n",
    "    try:\n",
    "        sm_client.delete_pipeline(PipelineName=\"LiverDiseasePipeline\")\n",
    "        print(\"Old pipeline deleted â€” recreating with updated definition â€¦\")\n",
    "        time.sleep(3)   # brief pause for deletion to propagate\n",
    "    except sm_client.exceptions.ResourceNotFound:\n",
    "        pass   # doesn't exist yet, nothing to delete\n",
    "\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    print(\"âœ… Pipeline 'LiverDiseasePipeline' created with built-in XGBoost algorithm.\")\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5.  TRAINING SCRIPTS  (written to disk before pipeline runs)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "XGB_TRAIN_SCRIPT = \"\"\"\n",
    "import argparse, os, glob, traceback\n",
    "import pandas as pd, numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def find_csv(directory):\n",
    "    \\\"\\\"\\\"Find the first CSV in a directory regardless of filename.\\\"\\\"\\\"\n",
    "    files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV found in {directory}. Contents: {os.listdir(directory)}\")\n",
    "    print(f\"  Found data file: {files[0]}\")\n",
    "    return files[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--max_depth\",   type=int,   default=5)\n",
    "        parser.add_argument(\"--eta\",         type=float, default=0.2)\n",
    "        parser.add_argument(\"--subsample\",   type=float, default=0.8)\n",
    "        parser.add_argument(\"--num_round\",   type=int,   default=100)\n",
    "        parser.add_argument(\"--objective\",   type=str,   default=\"binary:logistic\")\n",
    "        parser.add_argument(\"--eval_metric\", type=str,   default=\"auc\")\n",
    "        parser.add_argument(\"--model-dir\",   type=str,   default=os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "        parser.add_argument(\"--train\",       type=str,   default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "        parser.add_argument(\"--test\",        type=str,   default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        print(f\"Args: {args}\")\n",
    "        print(f\"Train dir: {args.train}  contents: {os.listdir(args.train) if args.train else 'None'}\")\n",
    "        if args.test:\n",
    "            print(f\"Test  dir: {args.test}   contents: {os.listdir(args.test)}\")\n",
    "\n",
    "        # â”€â”€ Load training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        train = pd.read_csv(find_csv(args.train), header=None)\n",
    "        print(f\"Train shape: {train.shape}\")\n",
    "\n",
    "        X_tr = train.iloc[:, 1:].values.astype(float)\n",
    "        y_tr = train.iloc[:, 0].values.astype(float)\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "\n",
    "        # â”€â”€ Load test data (optional â€“ used for early stopping) â”€â”€\n",
    "        evals = [(dtrain, \"train\")]\n",
    "        if args.test and os.path.isdir(args.test):\n",
    "            test  = pd.read_csv(find_csv(args.test), header=None)\n",
    "            print(f\"Test shape: {test.shape}\")\n",
    "            X_te  = test.iloc[:, 1:].values.astype(float)\n",
    "            y_te  = test.iloc[:, 0].values.astype(float)\n",
    "            dtest = xgb.DMatrix(X_te, label=y_te)\n",
    "            evals.append((dtest, \"test\"))\n",
    "        else:\n",
    "            print(\"No test channel found â€” training without early stopping.\")\n",
    "            dtest = None\n",
    "\n",
    "        # â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        params = {\n",
    "            \"max_depth\":   args.max_depth,\n",
    "            \"eta\":         args.eta,\n",
    "            \"subsample\":   args.subsample,\n",
    "            \"objective\":   args.objective,\n",
    "            \"eval_metric\": args.eval_metric,\n",
    "            \"seed\":        42,\n",
    "        }\n",
    "        print(f\"Training params: {params}\")\n",
    "\n",
    "        callbacks = []\n",
    "        if dtest is not None:\n",
    "            callbacks = [xgb.callback.EarlyStopping(rounds=10)]\n",
    "\n",
    "        model = xgb.train(\n",
    "            params, dtrain, args.num_round,\n",
    "            evals=evals,\n",
    "            callbacks=callbacks,\n",
    "            verbose_eval=10\n",
    "        )\n",
    "\n",
    "        # â”€â”€ Evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if dtest is not None:\n",
    "            preds = (model.predict(dtest) >= 0.5).astype(int)\n",
    "            acc   = accuracy_score(y_te, preds)\n",
    "            auc   = roc_auc_score(y_te, model.predict(dtest))\n",
    "            print(f\"XGBoost â†’ Accuracy: {acc:.4f}  AUC: {auc:.4f}\")\n",
    "\n",
    "        # â”€â”€ Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        os.makedirs(args.model_dir, exist_ok=True)\n",
    "        out_path = os.path.join(args.model_dir, \"xgboost-model\")\n",
    "        model.save_model(out_path)\n",
    "        print(f\"Model saved to {out_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"TRAINING FAILED: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"=\" * 60)\n",
    "        raise   # re-raise so SageMaker marks the job as failed\n",
    "\"\"\"\n",
    "\n",
    "RF_TRAIN_SCRIPT = \"\"\"\n",
    "import argparse, os, glob, traceback, joblib\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def find_csv(directory):\n",
    "    files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV found in {directory}. Contents: {os.listdir(directory)}\")\n",
    "    print(f\"  Found data file: {files[0]}\")\n",
    "    return files[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--n_estimators\", type=int, default=200)\n",
    "        parser.add_argument(\"--max_depth\",    type=int, default=10)\n",
    "        parser.add_argument(\"--model-dir\",    type=str, default=os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\"))\n",
    "        parser.add_argument(\"--train\",        type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        print(f\"Args: {args}\")\n",
    "        print(f\"Train dir: {args.train}  contents: {os.listdir(args.train) if args.train else 'None'}\")\n",
    "\n",
    "        data = pd.read_csv(find_csv(args.train), header=None)\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "        X = data.iloc[:, 1:].values.astype(float)\n",
    "        y = data.iloc[:, 0].values.astype(float)\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=args.n_estimators,\n",
    "            max_depth=args.max_depth,\n",
    "            random_state=42, n_jobs=-1\n",
    "        )\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        acc = accuracy_score(y, clf.predict(X))\n",
    "        print(f\"Random Forest â†’ Train Accuracy: {acc:.4f}\")\n",
    "\n",
    "        os.makedirs(args.model_dir, exist_ok=True)\n",
    "        out_path = os.path.join(args.model_dir, \"rf_model.joblib\")\n",
    "        joblib.dump(clf, out_path)\n",
    "        print(f\"Model saved to {out_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"TRAINING FAILED: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"=\" * 60)\n",
    "        raise\n",
    "\"\"\"\n",
    "\n",
    "EVALUATE_SCRIPT = \"\"\"\n",
    "import subprocess, sys\n",
    "\n",
    "# Install xgboost at runtime â€” it is NOT pre-installed in the SKLearn processing container\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost==1.5.2\", \"--quiet\"])\n",
    "\n",
    "import os, json, tarfile, glob, traceback\n",
    "import pandas as pd, numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "def list_dir(path):\n",
    "    \\\"\\\"\\\"Print directory contents for debugging.\\\"\\\"\\\"\n",
    "    if os.path.isdir(path):\n",
    "        print(f\"  Contents of {path}: {os.listdir(path)}\")\n",
    "    else:\n",
    "        print(f\"  Path does not exist: {path}\")\n",
    "\n",
    "def extract_tar(model_dir):\n",
    "    \\\"\\\"\\\"Extract model.tar.gz and return the extracted directory path.\\\"\\\"\\\"\n",
    "    # SageMaker puts the model artifact at either:\n",
    "    #   <dir>/model.tar.gz          (S3 single-file download)\n",
    "    #   <dir>/<job-name>/model.tar.gz  (rare, nested)\n",
    "    tars = glob.glob(os.path.join(model_dir, \"**\", \"model.tar.gz\"), recursive=True)\n",
    "    if not tars:\n",
    "        raise FileNotFoundError(f\"No model.tar.gz found under {model_dir}\")\n",
    "    tar_path = tars[0]\n",
    "    extract_dir = os.path.dirname(tar_path)\n",
    "    print(f\"  Extracting {tar_path} â†’ {extract_dir}\")\n",
    "    with tarfile.open(tar_path) as t:\n",
    "        t.extractall(extract_dir)\n",
    "    print(f\"  After extract: {os.listdir(extract_dir)}\")\n",
    "    return extract_dir\n",
    "\n",
    "def load_xgb(model_dir):\n",
    "    extract_dir = extract_tar(model_dir)\n",
    "    # Built-in XGBoost saves as 'xgboost-model'\n",
    "    candidates = glob.glob(os.path.join(extract_dir, \"xgboost-model*\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No xgboost-model file found in {extract_dir}\")\n",
    "    model_file = candidates[0]\n",
    "    print(f\"  Loading XGBoost model: {model_file}\")\n",
    "    m = xgb.Booster()\n",
    "    m.load_model(model_file)\n",
    "    return m\n",
    "\n",
    "def load_rf(model_dir):\n",
    "    extract_dir = extract_tar(model_dir)\n",
    "    candidates = glob.glob(os.path.join(extract_dir, \"*.joblib\"))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No .joblib file found in {extract_dir}\")\n",
    "    model_file = candidates[0]\n",
    "    print(f\"  Loading RF model: {model_file}\")\n",
    "    return joblib.load(model_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # â”€â”€ Diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"=== EVALUATE MODELS ===\")\n",
    "        list_dir(\"/opt/ml/processing/model/xgboost\")\n",
    "        list_dir(\"/opt/ml/processing/model/rf\")\n",
    "        list_dir(\"/opt/ml/processing/test\")\n",
    "\n",
    "        # â”€â”€ Load test data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        test_files = glob.glob(\"/opt/ml/processing/test/*.csv\")\n",
    "        if not test_files:\n",
    "            raise FileNotFoundError(\"No test CSV found in /opt/ml/processing/test/\")\n",
    "        print(f\"  Test file: {test_files[0]}\")\n",
    "        test_df = pd.read_csv(test_files[0], header=None)\n",
    "        print(f\"  Test shape: {test_df.shape}\")\n",
    "\n",
    "        X_te = test_df.iloc[:, 1:].values.astype(float)\n",
    "        y_te = test_df.iloc[:, 0].values.astype(float)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # â”€â”€ XGBoost evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        try:\n",
    "            print(\"\\\\nLoading XGBoost model ...\")\n",
    "            xgb_model = load_xgb(\"/opt/ml/processing/model/xgboost\")\n",
    "            dtest     = xgb.DMatrix(X_te)\n",
    "            xgb_prob  = xgb_model.predict(dtest)\n",
    "            xgb_pred  = (xgb_prob >= 0.5).astype(int)\n",
    "            results[\"xgboost\"] = {\n",
    "                \"accuracy\":  round(float(accuracy_score(y_te, xgb_pred)),  4),\n",
    "                \"auc\":       round(float(roc_auc_score(y_te, xgb_prob)),   4),\n",
    "                \"precision\": round(float(precision_score(y_te, xgb_pred)), 4),\n",
    "                \"recall\":    round(float(recall_score(y_te, xgb_pred)),    4),\n",
    "                \"f1\":        round(float(f1_score(y_te, xgb_pred)),        4),\n",
    "            }\n",
    "            print(f\"  XGBoost metrics: {results['xgboost']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  XGBoost eval failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            results[\"xgboost\"] = {\"error\": str(e), \"accuracy\": 0.0}\n",
    "\n",
    "        # â”€â”€ Random Forest evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        try:\n",
    "            print(\"\\\\nLoading Random Forest model ...\")\n",
    "            rf_model = load_rf(\"/opt/ml/processing/model/rf\")\n",
    "            rf_pred  = rf_model.predict(X_te)\n",
    "            rf_prob  = rf_model.predict_proba(X_te)[:, 1]\n",
    "            results[\"random_forest\"] = {\n",
    "                \"accuracy\":  round(float(accuracy_score(y_te, rf_pred)),  4),\n",
    "                \"auc\":       round(float(roc_auc_score(y_te, rf_prob)),   4),\n",
    "                \"precision\": round(float(precision_score(y_te, rf_pred)), 4),\n",
    "                \"recall\":    round(float(recall_score(y_te, rf_pred)),    4),\n",
    "                \"f1\":        round(float(f1_score(y_te, rf_pred)),        4),\n",
    "            }\n",
    "            print(f\"  RF metrics: {results['random_forest']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  RF eval failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            results[\"random_forest\"] = {\"error\": str(e), \"accuracy\": 0.0}\n",
    "\n",
    "        # â”€â”€ Best model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        best = max(results, key=lambda k: results[k].get(\"accuracy\", 0.0))\n",
    "        results[\"best_model\"] = best\n",
    "        print(f\"\\\\nBest model: {best}\")\n",
    "        print(json.dumps(results, indent=2))\n",
    "\n",
    "        # â”€â”€ Write evaluation report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        os.makedirs(\"/opt/ml/processing/evaluation\", exist_ok=True)\n",
    "        out_path = \"/opt/ml/processing/evaluation/evaluation.json\"\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\\\nEvaluation report written to {out_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"EVALUATION FAILED: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"=\" * 60)\n",
    "        raise\n",
    "\"\"\"\n",
    "\n",
    "def save_training_scripts():\n",
    "    scripts = {\n",
    "        \"xgb_train.py\":   XGB_TRAIN_SCRIPT,\n",
    "        \"rf_train.py\":    RF_TRAIN_SCRIPT,\n",
    "        \"evaluate.py\":    EVALUATE_SCRIPT,\n",
    "    }\n",
    "    for fname, content in scripts.items():\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(content)\n",
    "        print(f\"Script saved â†’ {fname}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6.  MODEL COMPARISON  (standalone, outside pipeline)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def compare_models_local(train_csv: str, test_csv: str) -> dict:\n",
    "    \"\"\"\n",
    "    Quick local comparison of XGBoost vs RF vs Logistic Regression.\n",
    "    Useful for rapid experimentation before launching full pipeline.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import xgboost as xgb\n",
    "\n",
    "    train = pd.read_csv(train_csv, header=None, encoding=\"latin-1\")\n",
    "    test  = pd.read_csv(test_csv,  header=None, encoding=\"latin-1\")\n",
    "    X_tr, y_tr = train.iloc[:, 1:].values, train.iloc[:, 0].values\n",
    "    X_te, y_te = test.iloc[:, 1:].values,  test.iloc[:, 0].values\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\":       RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "        \"Gradient Boosting\":   GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\":             xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                                  eval_metric=\"logloss\",\n",
    "                                                  random_state=42),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    print(f\"\\n{'Model':<25} {'Accuracy':>10} {'AUC':>10} {'F1':>10} {'CV-AUC (5-fold)':>18}\")\n",
    "    print(\"â”€\" * 80)\n",
    "\n",
    "    for name, m in models.items():\n",
    "        m.fit(X_tr, y_tr)\n",
    "        preds = m.predict(X_te)\n",
    "        proba = m.predict_proba(X_te)[:, 1]\n",
    "        cv_auc = cross_val_score(m, X_tr, y_tr, cv=5, scoring=\"roc_auc\").mean()\n",
    "\n",
    "        acc = accuracy_score(y_te, preds)\n",
    "        auc = roc_auc_score(y_te, proba)\n",
    "        f1  = f1_score(y_te, preds)\n",
    "\n",
    "        results[name] = {\"accuracy\": acc, \"auc\": auc, \"f1\": f1, \"cv_auc\": cv_auc}\n",
    "        print(f\"{name:<25} {acc:>10.4f} {auc:>10.4f} {f1:>10.4f} {cv_auc:>18.4f}\")\n",
    "\n",
    "    best = max(results, key=lambda k: results[k][\"auc\"])\n",
    "    print(f\"\\nğŸ† Best model by AUC: {best} ({results[best]['auc']:.4f})\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7.  HYPERPARAMETER TUNING  (Automatic Model Tuning)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def run_hyperparameter_tuning(train_s3_uri: str) -> HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    SageMaker Automatic Model Tuning for XGBoost (built-in algorithm).\n",
    "    Searches across eta, max_depth, subsample for best AUC.\n",
    "    \"\"\"\n",
    "    xgb_image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\", region=region, version=\"1.5-1\"\n",
    "    )\n",
    "    xgb_estimator = Estimator(\n",
    "        image_uri=xgb_image_uri,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        output_path=f\"{OUTPUT_PREFIX}/tuning\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters={\"num_round\": 100, \"objective\": \"binary:logistic\", \"eval_metric\": \"auc\"}\n",
    "    )\n",
    "\n",
    "    tuner = HyperparameterTuner(\n",
    "        estimator=xgb_estimator,\n",
    "        objective_metric_name=\"validation:auc\",\n",
    "        hyperparameter_ranges={\n",
    "            \"eta\":       ContinuousParameter(0.01, 0.3),\n",
    "            \"max_depth\": IntegerParameter(3, 10),\n",
    "            \"subsample\": ContinuousParameter(0.5, 1.0),\n",
    "        },\n",
    "        max_jobs=15,\n",
    "        max_parallel_jobs=3,\n",
    "        strategy=\"Bayesian\"\n",
    "    )\n",
    "\n",
    "    tuner.fit(\n",
    "        inputs={\n",
    "            \"train\":      TrainingInput(train_s3_uri, content_type=\"text/csv\"),\n",
    "            \"validation\": TrainingInput(train_s3_uri, content_type=\"text/csv\"),\n",
    "        },\n",
    "        wait=False\n",
    "    )\n",
    "    print(f\"Tuning job started: {tuner.latest_tuning_job.name}\")\n",
    "    return tuner\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8.  MODEL DEPLOYMENT\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "INFERENCE_SCRIPT = \"\"\"\n",
    "import os, io, json, tarfile\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(os.path.join(model_dir, \"xgboost-model\"))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"text/csv\":\n",
    "        data = np.array([float(x) for x in request_body.strip().split(\",\")])\n",
    "        return xgb.DMatrix(data.reshape(1, -1))\n",
    "    elif request_content_type == \"application/json\":\n",
    "        d = json.loads(request_body)\n",
    "        data = np.array(d[\"features\"])\n",
    "        return xgb.DMatrix(data.reshape(1, -1))\n",
    "    raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    probability = float(model.predict(input_data)[0])\n",
    "    return probability\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    result = {\n",
    "        \"probability\": round(prediction, 4),\n",
    "        \"prediction\":  int(prediction >= 0.5),\n",
    "        \"label\":       \"liver_disease\" if prediction >= 0.5 else \"no_disease\"\n",
    "    }\n",
    "    return json.dumps(result), \"application/json\"\n",
    "\"\"\"\n",
    "\n",
    "def deploy_model(model_data_s3: str, endpoint_name: str = \"liver-disease-endpoint\") -> sagemaker.predictor.Predictor:\n",
    "    \"\"\"\n",
    "    Deploy best XGBoost model to a real-time SageMaker endpoint\n",
    "    with data capture enabled for monitoring.\n",
    "    \"\"\"\n",
    "    with open(\"inference.py\", \"w\") as f:\n",
    "        f.write(INFERENCE_SCRIPT)\n",
    "\n",
    "    # Data Capture Config (required for Model Monitor)\n",
    "    data_capture_config = DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=f\"s3://{BUCKET}/{PREFIX}/data-capture\",\n",
    "        capture_options=[\"REQUEST\", \"RESPONSE\"]\n",
    "    )\n",
    "\n",
    "    xgb_image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\", region=region, version=\"1.5-1\"\n",
    "    )\n",
    "    model = sagemaker.Model(\n",
    "        image_uri=xgb_image_uri,\n",
    "        model_data=model_data_s3,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\"}\n",
    "    )\n",
    "\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.large\",\n",
    "        endpoint_name=endpoint_name,\n",
    "        data_capture_config=data_capture_config\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Endpoint deployed: {endpoint_name}\")\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def test_endpoint(predictor, sample_features: list):\n",
    "    \"\"\"Send a test prediction to the deployed endpoint.\"\"\"\n",
    "    import csv, io\n",
    "    payload = \",\".join(map(str, sample_features))\n",
    "    response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "    print(f\"Prediction: {response}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9.  MODEL MONITORING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def setup_model_monitoring(endpoint_name: str, baseline_s3_uri: str):\n",
    "    \"\"\"\n",
    "    Sets up two monitoring schedules:\n",
    "      1. Data Quality Monitor  â€“ detects data drift / schema violations\n",
    "      2. Model Quality Monitor â€“ tracks accuracy / AUC over time\n",
    "    \"\"\"\n",
    "\n",
    "    # â”€â”€ Data Quality Monitor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    data_monitor = DefaultModelMonitor(\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=20,\n",
    "        max_runtime_in_seconds=3600,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # Suggest baseline constraints & statistics from training data\n",
    "    data_monitor.suggest_baseline(\n",
    "        baseline_dataset=baseline_s3_uri,\n",
    "        dataset_format=DatasetFormat.csv(header=True),\n",
    "        output_s3_uri=f\"s3://{BUCKET}/{PREFIX}/monitoring/baseline\",\n",
    "        wait=True\n",
    "    )\n",
    "    print(\"Baseline statistics computed.\")\n",
    "\n",
    "    # Schedule hourly monitoring\n",
    "    data_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=\"liver-data-quality-monitor\",\n",
    "        endpoint_input=endpoint_name,\n",
    "        output_s3_uri=f\"s3://{BUCKET}/{PREFIX}/monitoring/data-quality\",\n",
    "        statistics=data_monitor.baseline_statistics(),\n",
    "        constraints=data_monitor.suggested_constraints(),\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True\n",
    "    )\n",
    "    print(\"Data Quality Monitor schedule created (hourly).\")\n",
    "\n",
    "    # â”€â”€ Model Quality Monitor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model_quality_monitor = ModelQualityMonitor(\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size_in_gb=20,\n",
    "        max_runtime_in_seconds=1800,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    model_quality_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=\"liver-model-quality-monitor\",\n",
    "        endpoint_input=sagemaker.model_monitor.EndpointInput(\n",
    "            endpoint_name=endpoint_name,\n",
    "            destination=\"/opt/ml/processing/input_data\",\n",
    "            start_time_offset=\"-PT1H\",\n",
    "            end_time_offset=\"-PT0H\",\n",
    "            probability_threshold_attribute=0.5,\n",
    "        ),\n",
    "        ground_truth_input=f\"s3://{BUCKET}/{PREFIX}/ground-truth/\",\n",
    "        problem_type=\"BinaryClassification\",\n",
    "        output_s3_uri=f\"s3://{BUCKET}/{PREFIX}/monitoring/model-quality\",\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True\n",
    "    )\n",
    "    print(\"Model Quality Monitor schedule created (hourly).\")\n",
    "\n",
    "    return data_monitor, model_quality_monitor\n",
    "\n",
    "\n",
    "def check_monitoring_violations(schedule_name: str):\n",
    "    \"\"\"List recent monitoring violations.\"\"\"\n",
    "    executions = sm_client.list_monitoring_executions(\n",
    "        MonitoringScheduleName=schedule_name,\n",
    "        SortBy=\"ScheduledTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "        MaxResults=5\n",
    "    )[\"MonitoringExecutionSummaries\"]\n",
    "\n",
    "    print(f\"\\nLatest executions for '{schedule_name}':\")\n",
    "    for ex in executions:\n",
    "        print(f\"  {ex['ScheduledTime']} â†’ {ex['MonitoringExecutionStatus']}\")\n",
    "        if ex.get(\"FailureReason\"):\n",
    "            print(f\"    âš  {ex['FailureReason']}\")\n",
    "\n",
    "    return executions\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 10.  CI/CD PIPELINE  (CodePipeline + SageMaker)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "BUILDSPEC_YAML = \"\"\"\n",
    "version: 0.2\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      python: 3.9\n",
    "    commands:\n",
    "      - pip install sagemaker boto3 pandas scikit-learn xgboost\n",
    "\n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo \"Running unit tests ...\"\n",
    "      - python -m pytest tests/ -v --tb=short\n",
    "\n",
    "  build:\n",
    "    commands:\n",
    "      - echo \"Triggering SageMaker Pipeline ...\"\n",
    "      - python cicd_trigger.py\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo \"Build complete. Checking pipeline status ...\"\n",
    "      - python cicd_check_status.py\n",
    "\n",
    "artifacts:\n",
    "  files:\n",
    "    - \"**/*\"\n",
    "\"\"\"\n",
    "\n",
    "CICD_TRIGGER_SCRIPT = \"\"\"\n",
    "import boto3, json\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "response = sm.start_pipeline_execution(\n",
    "    PipelineName=\"LiverDiseasePipeline\",\n",
    "    PipelineExecutionDisplayName=\"cicd-triggered\",\n",
    "    PipelineParameters=[\n",
    "        {\"Name\": \"ModelApprovalStatus\",  \"Value\": \"Approved\"},\n",
    "        {\"Name\": \"AccuracyThreshold\",    \"Value\": \"0.75\"},\n",
    "    ]\n",
    ")\n",
    "execution_arn = response[\"PipelineExecutionArn\"]\n",
    "print(f\"Pipeline execution started: {execution_arn}\")\n",
    "\n",
    "# Write ARN for next step\n",
    "with open(\"execution_arn.txt\", \"w\") as f:\n",
    "    f.write(execution_arn)\n",
    "\"\"\"\n",
    "\n",
    "CICD_STATUS_SCRIPT = \"\"\"\n",
    "import boto3, time\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "with open(\"execution_arn.txt\") as f:\n",
    "    arn = f.read().strip()\n",
    "\n",
    "for _ in range(60):        # poll for up to 60 minutes\n",
    "    r      = sm.describe_pipeline_execution(PipelineExecutionArn=arn)\n",
    "    status = r[\"PipelineExecutionStatus\"]\n",
    "    print(f\"Status: {status}\")\n",
    "    if status in (\"Succeeded\", \"Failed\", \"Stopped\"):\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if status != \"Succeeded\":\n",
    "    raise SystemExit(f\"Pipeline did not succeed: {status}\")\n",
    "print(\"Pipeline succeeded! Model ready for deployment.\")\n",
    "\"\"\"\n",
    "\n",
    "UNIT_TEST_SCRIPT = \"\"\"\n",
    "import pytest, pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def test_preprocessing_no_nulls():\n",
    "    df = pd.read_csv(\"Liver_Patient_Dataset__LPD__train.csv\", encoding=\"latin-1\")\n",
    "    df.dropna(inplace=True)\n",
    "    assert df.isnull().sum().sum() == 0\n",
    "\n",
    "def test_target_binary():\n",
    "    df = pd.read_csv(\"Liver_Patient_Dataset__LPD__train.csv\", encoding=\"latin-1\")\n",
    "    df.dropna(inplace=True)\n",
    "    df[\"Result\"] = (df[\"Result\"] == 1).astype(int)\n",
    "    assert set(df[\"Result\"].unique()).issubset({0, 1})\n",
    "\n",
    "def test_feature_count():\n",
    "    df = pd.read_csv(\"Liver_Patient_Dataset__LPD__train.csv\", encoding=\"latin-1\")\n",
    "    df.dropna(inplace=True)\n",
    "    # 11 original + 3 engineered - 1 target = 13 features\n",
    "    assert df.shape[1] >= 11\n",
    "\n",
    "def test_scaler_mean_zero():\n",
    "    data = np.random.randn(100, 5)\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    assert abs(scaled.mean()) < 0.1\n",
    "\"\"\"\n",
    "\n",
    "def setup_cicd_files():\n",
    "    \"\"\"Write all CI/CD related files to disk.\"\"\"\n",
    "    files = {\n",
    "        \"buildspec.yml\":         BUILDSPEC_YAML,\n",
    "        \"cicd_trigger.py\":       CICD_TRIGGER_SCRIPT,\n",
    "        \"cicd_check_status.py\":  CICD_STATUS_SCRIPT,\n",
    "        \"tests/test_pipeline.py\": UNIT_TEST_SCRIPT,\n",
    "    }\n",
    "    import os\n",
    "    os.makedirs(\"tests\", exist_ok=True)\n",
    "    for path, content in files.items():\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(content)\n",
    "    print(\"CI/CD files created.\")\n",
    "\n",
    "\n",
    "def create_codepipeline(github_repo: str, github_branch: str = \"main\"):\n",
    "    \"\"\"\n",
    "    Creates an AWS CodePipeline that:\n",
    "      Source  â†’ GitHub (on push to main)\n",
    "      Build   â†’ CodeBuild (runs buildspec.yml)\n",
    "      Deploy  â†’ triggers SageMaker Pipeline\n",
    "    \"\"\"\n",
    "    cp_client = boto_session.client(\"codepipeline\")\n",
    "\n",
    "    pipeline_definition = {\n",
    "        \"name\":    \"LiverDiseaseMLOps\",\n",
    "        \"roleArn\": role,\n",
    "        \"artifactStore\": {\n",
    "            \"type\":     \"S3\",\n",
    "            \"location\": BUCKET\n",
    "        },\n",
    "        \"stages\": [\n",
    "            {\n",
    "                \"name\": \"Source\",\n",
    "                \"actions\": [{\n",
    "                    \"name\":       \"GitHub_Source\",\n",
    "                    \"actionTypeId\": {\n",
    "                        \"category\": \"Source\",\n",
    "                        \"owner\":    \"ThirdParty\",\n",
    "                        \"provider\": \"GitHub\",\n",
    "                        \"version\":  \"1\"\n",
    "                    },\n",
    "                    \"configuration\": {\n",
    "                        \"Owner\":      github_repo.split(\"/\")[0],\n",
    "                        \"Repo\":       github_repo.split(\"/\")[1],\n",
    "                        \"Branch\":     github_branch,\n",
    "                        \"OAuthToken\": \"{{resolve:secretsmanager:github-token}}\"\n",
    "                    },\n",
    "                    \"outputArtifacts\": [{\"name\": \"SourceOutput\"}]\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Build\",\n",
    "                \"actions\": [{\n",
    "                    \"name\": \"CodeBuild\",\n",
    "                    \"actionTypeId\": {\n",
    "                        \"category\": \"Build\",\n",
    "                        \"owner\":    \"AWS\",\n",
    "                        \"provider\": \"CodeBuild\",\n",
    "                        \"version\":  \"1\"\n",
    "                    },\n",
    "                    \"configuration\": {\n",
    "                        \"ProjectName\": \"LiverDiseaseMLBuild\"\n",
    "                    },\n",
    "                    \"inputArtifacts\":  [{\"name\": \"SourceOutput\"}],\n",
    "                    \"outputArtifacts\": [{\"name\": \"BuildOutput\"}]\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        cp_client.create_pipeline(pipeline=pipeline_definition)\n",
    "        print(\"CodePipeline 'LiverDiseaseMLOps' created.\")\n",
    "    except cp_client.exceptions.PipelineNameInUseException:\n",
    "        cp_client.update_pipeline(pipeline=pipeline_definition)\n",
    "        print(\"CodePipeline 'LiverDiseaseMLOps' updated.\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 11.  CLOUDWATCH INFRASTRUCTURE MONITORING DASHBOARD\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def create_cloudwatch_dashboard(endpoint_name: str = \"liver-disease-endpoint\"):\n",
    "    \"\"\"\n",
    "    Creates a CloudWatch dashboard with widgets covering:\n",
    "      - Endpoint invocation metrics (latency, throughput, errors)\n",
    "      - Instance-level CPU / Memory utilisation\n",
    "      - Model Monitor violation counts\n",
    "      - Training job metrics (XGBoost AUC over rounds)\n",
    "      - SageMaker Pipeline execution status\n",
    "    \"\"\"\n",
    "    cw = boto_session.client(\"cloudwatch\")\n",
    "    dashboard_name = \"LiverDisease-MLOps-Dashboard\"\n",
    "\n",
    "    # Helper: metric widget definition\n",
    "    def metric_widget(title, metrics, period=300, stat=\"Average\",\n",
    "                      width=12, height=6, view=\"timeSeries\"):\n",
    "        return {\n",
    "            \"type\": \"metric\",\n",
    "            \"properties\": {\n",
    "                \"title\":   title,\n",
    "                \"view\":    view,\n",
    "                \"stacked\": False,\n",
    "                \"stat\":    stat,\n",
    "                \"period\":  period,\n",
    "                \"metrics\": metrics,\n",
    "                \"width\":   width,\n",
    "                \"height\":  height,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Helper: alarm widget\n",
    "    def alarm_widget(title, alarm_arns, width=12, height=3):\n",
    "        return {\n",
    "            \"type\": \"alarm\",\n",
    "            \"properties\": {\n",
    "                \"title\":  title,\n",
    "                \"alarms\": alarm_arns,\n",
    "                \"width\":  width,\n",
    "                \"height\": height,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    widgets = [\n",
    "        # â”€â”€ Row 1: Endpoint Invocation Health â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        metric_widget(\n",
    "            title=\"Endpoint â€“ Invocations per Minute\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"Invocations\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "                [\"AWS/SageMaker\", \"InvocationsPerInstance\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "            ],\n",
    "            stat=\"Sum\"\n",
    "        ),\n",
    "        metric_widget(\n",
    "            title=\"Endpoint â€“ Model Latency (ms)\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"ModelLatency\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "                [\"AWS/SageMaker\", \"OverheadLatency\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "            ]\n",
    "        ),\n",
    "        # â”€â”€ Row 2: Error Rates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        metric_widget(\n",
    "            title=\"Endpoint â€“ 4xx / 5xx Errors\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"Invocation4XXErrors\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "                [\"AWS/SageMaker\", \"Invocation5XXErrors\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "            ],\n",
    "            stat=\"Sum\"\n",
    "        ),\n",
    "        # â”€â”€ Row 3: Instance Resource Utilisation â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        metric_widget(\n",
    "            title=\"Endpoint Instance â€“ CPU & Memory Utilisation (%)\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"CPUUtilization\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "                [\"AWS/SageMaker\", \"MemoryUtilization\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "            ]\n",
    "        ),\n",
    "        metric_widget(\n",
    "            title=\"Endpoint Instance â€“ Disk Utilisation (%)\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"DiskUtilization\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", \"AllTraffic\"],\n",
    "            ]\n",
    "        ),\n",
    "        # â”€â”€ Row 4: Model Monitor Violations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        metric_widget(\n",
    "            title=\"Data Quality Monitor â€“ Violation Count\",\n",
    "            metrics=[\n",
    "                [\"/aws/sagemaker/Endpoints/data-metrics\", \"feature_baseline_drift_violations\",\n",
    "                 \"MonitoringSchedule\", \"liver-data-quality-monitor\"],\n",
    "            ],\n",
    "            stat=\"Sum\"\n",
    "        ),\n",
    "        metric_widget(\n",
    "            title=\"Model Quality Monitor â€“ Violation Count\",\n",
    "            metrics=[\n",
    "                [\"/aws/sagemaker/Endpoints/data-metrics\", \"metric_violations\",\n",
    "                 \"MonitoringSchedule\", \"liver-model-quality-monitor\"],\n",
    "            ],\n",
    "            stat=\"Sum\"\n",
    "        ),\n",
    "        # â”€â”€ Row 5: Feature Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        metric_widget(\n",
    "            title=\"Feature Store â€“ Put / Get Record Latency (ms)\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"PutRecord.Latency\",\n",
    "                 \"FeatureGroupName\", FEATURE_GROUP_NAME],\n",
    "                [\"AWS/SageMaker\", \"GetRecord.Latency\",\n",
    "                 \"FeatureGroupName\", FEATURE_GROUP_NAME],\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    body = json.dumps({\"widgets\": widgets})\n",
    "    dashboard_created = False\n",
    "    try:\n",
    "        cw.put_dashboard(DashboardName=dashboard_name, DashboardBody=body)\n",
    "        dashboard_created = True\n",
    "        print(f\"âœ… CloudWatch dashboard created: '{dashboard_name}'\")\n",
    "        print(f\"   â†’ https://console.aws.amazon.com/cloudwatch/home#dashboards:name={dashboard_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš   Could not create CloudWatch dashboard (IAM permission missing: cloudwatch:PutDashboard)\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print()\n",
    "        print(\"   â”€â”€ To create the dashboard manually â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        print(\"   1. Open AWS Console â†’ CloudWatch â†’ Dashboards â†’ Create dashboard\")\n",
    "        print(f\"   2. Name it: {dashboard_name}\")\n",
    "        print(\"   3. Add widgets for these metrics:\")\n",
    "        print(f\"      â€¢ AWS/SageMaker â†’ EndpointName={endpoint_name} â†’ Invocations, ModelLatency\")\n",
    "        print(f\"      â€¢ AWS/SageMaker â†’ EndpointName={endpoint_name} â†’ CPUUtilization, MemoryUtilization\")\n",
    "        print(f\"      â€¢ AWS/SageMaker â†’ EndpointName={endpoint_name} â†’ Invocation4XXErrors, Invocation5XXErrors\")\n",
    "        print(\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "\n",
    "    # â”€â”€ CloudWatch Alarms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    alarms = [\n",
    "        {\n",
    "            \"AlarmName\":          \"LiverDisease-HighLatency\",\n",
    "            \"AlarmDescription\":   \"Model latency exceeds 500ms\",\n",
    "            \"MetricName\":         \"ModelLatency\",\n",
    "            \"Namespace\":          \"AWS/SageMaker\",\n",
    "            \"Dimensions\":         [{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                                   {\"Name\": \"VariantName\",  \"Value\": \"AllTraffic\"}],\n",
    "            \"Statistic\":          \"Average\",\n",
    "            \"Period\":             60,\n",
    "            \"EvaluationPeriods\":  3,\n",
    "            \"Threshold\":          500000,   # microseconds\n",
    "            \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "            \"TreatMissingData\":   \"notBreaching\",\n",
    "        },\n",
    "        {\n",
    "            \"AlarmName\":          \"LiverDisease-5xxErrors\",\n",
    "            \"AlarmDescription\":   \"Endpoint returning 5xx errors\",\n",
    "            \"MetricName\":         \"Invocation5XXErrors\",\n",
    "            \"Namespace\":          \"AWS/SageMaker\",\n",
    "            \"Dimensions\":         [{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                                   {\"Name\": \"VariantName\",  \"Value\": \"AllTraffic\"}],\n",
    "            \"Statistic\":          \"Sum\",\n",
    "            \"Period\":             60,\n",
    "            \"EvaluationPeriods\":  2,\n",
    "            \"Threshold\":          1,\n",
    "            \"ComparisonOperator\": \"GreaterThanOrEqualToThreshold\",\n",
    "            \"TreatMissingData\":   \"notBreaching\",\n",
    "        },\n",
    "        {\n",
    "            \"AlarmName\":          \"LiverDisease-HighCPU\",\n",
    "            \"AlarmDescription\":   \"Endpoint CPU > 80%\",\n",
    "            \"MetricName\":         \"CPUUtilization\",\n",
    "            \"Namespace\":          \"AWS/SageMaker\",\n",
    "            \"Dimensions\":         [{\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                                   {\"Name\": \"VariantName\",  \"Value\": \"AllTraffic\"}],\n",
    "            \"Statistic\":          \"Average\",\n",
    "            \"Period\":             300,\n",
    "            \"EvaluationPeriods\":  2,\n",
    "            \"Threshold\":          80,\n",
    "            \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "            \"TreatMissingData\":   \"notBreaching\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for alarm in alarms:\n",
    "        try:\n",
    "            cw.put_metric_alarm(**alarm)\n",
    "            print(f\"  Alarm set â†’ {alarm['AlarmName']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš   Alarm '{alarm['AlarmName']}' skipped (check IAM): {e}\")\n",
    "\n",
    "    return dashboard_name\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 12.  CI/CD DAG  â€“ SUCCESS & FAILED STATE HANDLER\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def visualise_pipeline_dag(execution_arn: str):\n",
    "    \"\"\"\n",
    "    Prints a text-based DAG of the pipeline execution showing\n",
    "    each step's status.  Clearly shows SUCCESS and FAILED states\n",
    "    with actionable failure messages.\n",
    "    \"\"\"\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn,\n",
    "        SortOrder=\"Ascending\"\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "\n",
    "    STATUS_ICON = {\n",
    "        \"Succeeded\":  \"âœ…\",\n",
    "        \"Failed\":     \"âŒ\",\n",
    "        \"Executing\":  \"ğŸ”„\",\n",
    "        \"Stopped\":    \"â¹ï¸\",\n",
    "        \"Starting\":   \"â³\",\n",
    "        \"Skipped\":    \"â­ï¸\",\n",
    "    }\n",
    "\n",
    "    DAG_SHAPE = [\n",
    "        (\"LiverDiseasePreprocessing\", None),\n",
    "        (\"TrainXGBoost\",              \"LiverDiseasePreprocessing\"),\n",
    "        (\"TrainRandomForest\",         \"LiverDiseasePreprocessing\"),\n",
    "        (\"EvaluateModels\",            \"TrainXGBoost + TrainRandomForest\"),\n",
    "        (\"CheckAccuracy\",             \"EvaluateModels\"),\n",
    "        (\"RegisterLiverDiseaseModel\", \"CheckAccuracy\"),\n",
    "    ]\n",
    "\n",
    "    step_map = {s[\"StepName\"]: s for s in steps}\n",
    "\n",
    "    print(\"\\n\" + \"â•\" * 62)\n",
    "    print(\"  CI/CD PIPELINE DAG\")\n",
    "    print(\"â•\" * 62)\n",
    "\n",
    "    any_failed = False\n",
    "    for step_name, parent in DAG_SHAPE:\n",
    "        info   = step_map.get(step_name, {})\n",
    "        status = info.get(\"StepStatus\", \"NotStarted\")\n",
    "        icon   = STATUS_ICON.get(status, \"â¬œ\")\n",
    "        indent = \"      â””â”€â–º \" if parent else \"  \"\n",
    "\n",
    "        print(f\"{indent}{icon}  {step_name:<38} [{status}]\")\n",
    "\n",
    "        # Show failure reason inline\n",
    "        if status == \"Failed\":\n",
    "            any_failed = True\n",
    "            reason = info.get(\"FailureReason\", \"No reason provided\")\n",
    "            print(f\"           âš   Failure: {reason}\")\n",
    "\n",
    "        # Show condition branch outcome\n",
    "        if step_name == \"CheckAccuracy\" and status == \"Succeeded\":\n",
    "            metadata = info.get(\"Metadata\", {})\n",
    "            outcome  = metadata.get(\"Condition\", {}).get(\"Outcome\", \"unknown\")\n",
    "            print(f\"           â†³  Condition outcome: {outcome.upper()}\")\n",
    "            if outcome == \"false\":\n",
    "                print(\"           â†³  Model NOT registered (accuracy below threshold)\")\n",
    "\n",
    "    print(\"â•\" * 62)\n",
    "\n",
    "    # â”€â”€ Overall result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    execution_info = sm_client.describe_pipeline_execution(\n",
    "        PipelineExecutionArn=execution_arn\n",
    "    )\n",
    "    overall = execution_info[\"PipelineExecutionStatus\"]\n",
    "    print(f\"\\n  Overall Status : {STATUS_ICON.get(overall, '?')}  {overall}\")\n",
    "\n",
    "    if overall == \"Failed\" or any_failed:\n",
    "        print(\"\\n  â”€â”€ FAILURE REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        for step_name, _ in DAG_SHAPE:\n",
    "            info = step_map.get(step_name, {})\n",
    "            if info.get(\"StepStatus\") == \"Failed\":\n",
    "                print(f\"  Step  : {step_name}\")\n",
    "                print(f\"  Reason: {info.get('FailureReason', 'N/A')}\")\n",
    "                print(f\"  ARN   : {info.get('Metadata', {})}\")\n",
    "                print()\n",
    "        print(\"  â”€â”€ REMEDIATION HINTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        print(\"  1. Check CloudWatch Logs for the failed Processing/Training job\")\n",
    "        print(\"  2. Verify IAM role has s3:PutObject on the output bucket\")\n",
    "        print(\"  3. If CheckAccuracy failed: lower AccuracyThreshold parameter\")\n",
    "        print(\"  4. Re-run:  execution = pipeline.start(...)\")\n",
    "    else:\n",
    "        print(\"\\n  â”€â”€ SUCCESS SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        print(\"  All steps completed. Model registered in Model Registry.\")\n",
    "        print(\"  Next: approve model â†’ deploy endpoint â†’ enable monitoring.\")\n",
    "\n",
    "    print(\"â•\" * 62 + \"\\n\")\n",
    "    return overall\n",
    "\n",
    "\n",
    "def monitor_pipeline_execution(pipeline_name: str = \"LiverDiseasePipeline\",\n",
    "                                poll_seconds: int = 30):\n",
    "    \"\"\"\n",
    "    Starts the pipeline and live-polls it, printing DAG updates\n",
    "    until it reaches a terminal state (Succeeded / Failed / Stopped).\n",
    "    Handles both SUCCESS and FAILED outcomes with full reporting.\n",
    "    \"\"\"\n",
    "    # Start execution\n",
    "    response = sm_client.start_pipeline_execution(\n",
    "        PipelineName=pipeline_name,\n",
    "        PipelineParameters=[\n",
    "            {\"Name\": \"ModelApprovalStatus\", \"Value\": \"PendingManualApproval\"},\n",
    "            {\"Name\": \"AccuracyThreshold\",   \"Value\": \"0.75\"},\n",
    "        ]\n",
    "    )\n",
    "    arn = response[\"PipelineExecutionArn\"]\n",
    "    print(f\"Pipeline execution started:\\n  {arn}\\n\")\n",
    "\n",
    "    terminal = {\"Succeeded\", \"Failed\", \"Stopped\"}\n",
    "    while True:\n",
    "        status = sm_client.describe_pipeline_execution(\n",
    "            PipelineExecutionArn=arn\n",
    "        )[\"PipelineExecutionStatus\"]\n",
    "\n",
    "        visualise_pipeline_dag(arn)\n",
    "\n",
    "        if status in terminal:\n",
    "            break\n",
    "\n",
    "        print(f\"  Polling again in {poll_seconds}s â€¦\\n\")\n",
    "        time.sleep(poll_seconds)\n",
    "\n",
    "    # Final SNS notification (optional â€“ requires an SNS topic)\n",
    "    _send_pipeline_notification(arn, status)\n",
    "    return arn, status\n",
    "\n",
    "\n",
    "def _send_pipeline_notification(execution_arn: str, status: str):\n",
    "    \"\"\"SNS requires sns:CreateTopic + sns:Publish IAM permissions not available\n",
    "    on this SageMaker role. No-op here â€” silently skipped.\n",
    "    To enable in production: add SNS permissions to the IAM role, then\n",
    "    call sns.create_topic() + sns.publish() here.\n",
    "    \"\"\"\n",
    "    pass  # no-op â€” IAM permissions not available\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 13.  BATCH INFERENCE  +  ENDPOINT INVOCATION EXAMPLES\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def run_batch_transform(model_name: str, test_s3_uri: str):\n",
    "    \"\"\"\n",
    "    SageMaker Batch Transform job â€” runs inference on the full\n",
    "    test set in S3 and writes predictions back to S3.\n",
    "    No endpoint needed; ideal for offline/bulk scoring.\n",
    "    \"\"\"\n",
    "    transformer = sagemaker.transformer.Transformer(\n",
    "        model_name=model_name,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        output_path=f\"s3://{BUCKET}/{PREFIX}/batch-output\",\n",
    "        base_transform_job_name=\"liver-disease-batch\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        accept=\"text/csv\",\n",
    "        assemble_with=\"Line\",\n",
    "        strategy=\"MultiRecord\",\n",
    "    )\n",
    "\n",
    "    # data_type: use S3Prefix for directories, ManifestFile for single files\n",
    "    data_type = \"S3Prefix\" if not test_s3_uri.endswith(\".csv\") else \"S3Prefix\"\n",
    "\n",
    "    transformer.transform(\n",
    "        data=test_s3_uri,\n",
    "        data_type=data_type,\n",
    "        content_type=\"text/csv\",\n",
    "        split_type=\"Line\",\n",
    "        wait=True,\n",
    "        logs=False   # suppress verbose nginx/gunicorn logs\n",
    "    )\n",
    "\n",
    "    output_uri = transformer.output_path\n",
    "    print(f\"\\nâœ… Batch transform complete.\")\n",
    "    print(f\"   Output â†’ {output_uri}\")\n",
    "\n",
    "    # â”€â”€ Download & display sample predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    s3 = boto_session.resource(\"s3\")\n",
    "    bucket_obj = s3.Bucket(BUCKET)\n",
    "    out_prefix = f\"{PREFIX}/batch-output\"\n",
    "\n",
    "    print(\"\\nâ”€â”€ Sample Batch Predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(f\"{'Row':<6} {'Raw Score':>12} {'Prediction':>12} {'Label':>18}\")\n",
    "    print(\"â”€\" * 55)\n",
    "\n",
    "    for i, obj in enumerate(bucket_obj.objects.filter(Prefix=out_prefix)):\n",
    "        content = obj.get()[\"Body\"].read().decode(\"utf-8\")\n",
    "        for j, line in enumerate(content.strip().split(\"\\n\")[:10]):\n",
    "            score = float(line.strip())\n",
    "            pred  = int(score >= 0.5)\n",
    "            label = \"liver_disease\" if pred == 1 else \"no_disease\"\n",
    "            print(f\"{j:<6} {score:>12.4f} {pred:>12} {label:>18}\")\n",
    "        break   # just show first output file\n",
    "\n",
    "    return output_uri\n",
    "\n",
    "\n",
    "def invoke_endpoint_examples(endpoint_name: str = \"liver-disease-endpoint\"):\n",
    "    \"\"\"\n",
    "    Demonstrates three ways to call the deployed endpoint:\n",
    "      1. Single record (CSV)\n",
    "      2. Single record (JSON)\n",
    "      3. Batch of records via loop\n",
    "    Uses real feature values from the dataset.\n",
    "    \"\"\"\n",
    "    runtime = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "    # Sample patients from the dataset (post-preprocessing, normalised approx values)\n",
    "    sample_patients = [\n",
    "        # Age  Gender  TotalBili  DirBili  AlkPhos  SGPT   SGOT  TotProt  Albumin  AGRatio  BiliRatio  EnzRatio  AgeGrp\n",
    "        [65,    1,     0.7,       0.1,     187,     16,    18,   6.8,     3.3,     0.90,    0.14,      0.89,     2],  # likely disease\n",
    "        [32,    0,     0.9,       0.2,     200,     20,    22,   7.0,     3.5,     1.10,    0.22,      0.91,     1],  # likely no disease\n",
    "        [55,    1,     3.5,       1.2,     450,     120,   95,   5.5,     2.8,     0.70,    0.34,      1.26,     2],  # high risk\n",
    "    ]\n",
    "\n",
    "    labels = [\"Patient A (65M)\", \"Patient B (32F)\", \"Patient C (55M â€“ high risk)\"]\n",
    "\n",
    "    print(\"\\n\" + \"â•\" * 65)\n",
    "    print(\"  ENDPOINT INVOCATION EXAMPLES\")\n",
    "    print(\"â•\" * 65)\n",
    "\n",
    "    # â”€â”€ Method 1: CSV payload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ Method 1: CSV Payload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    for patient, label in zip(sample_patients, labels):\n",
    "        payload = \",\".join(map(str, patient))\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=payload\n",
    "        )\n",
    "        result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "        print(f\"  {label:<28} â†’ prob={result['probability']:.4f}  \"\n",
    "              f\"pred={result['prediction']}  ({result['label']})\")\n",
    "\n",
    "    # â”€â”€ Method 2: JSON payload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ Method 2: JSON Payload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    payload = json.dumps({\"features\": sample_patients[0]})\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=payload\n",
    "    )\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "    print(f\"  JSON response: {json.dumps(result, indent=4)}\")\n",
    "\n",
    "    # â”€â”€ Method 3: Batch loop with latency tracking â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ Method 3: Bulk Invocation (100 records) â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    latencies, predictions = [], []\n",
    "    for _ in range(100):\n",
    "        patient = sample_patients[_ % len(sample_patients)]\n",
    "        payload = \",\".join(map(str, patient))\n",
    "        t0 = time.time()\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=payload\n",
    "        )\n",
    "        latency_ms = (time.time() - t0) * 1000\n",
    "        result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "        latencies.append(latency_ms)\n",
    "        predictions.append(result[\"prediction\"])\n",
    "\n",
    "    disease_pct = sum(predictions) / len(predictions) * 100\n",
    "    print(f\"  Records invokedÃ©: 100\")\n",
    "    print(f\"  Avg latency  : {np.mean(latencies):.1f} ms\")\n",
    "    print(f\"  p95 latency  : {np.percentile(latencies, 95):.1f} ms\")\n",
    "    print(f\"  Max latency  : {np.max(latencies):.1f} ms\")\n",
    "    print(f\"  Disease rate : {disease_pct:.1f}%\")\n",
    "    print(\"â•\" * 65)\n",
    "\n",
    "    return predictions, latencies\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 14.  MONITORING REPORT GENERATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def generate_monitoring_report(endpoint_name: str = \"liver-disease-endpoint\"):\n",
    "    \"\"\"\n",
    "    Pulls the latest monitoring execution results from S3,\n",
    "    parses violations and statistics, and prints a full report.\n",
    "    Covers both Data Quality and Model Quality monitors.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"â•\" * 65)\n",
    "    print(\"  MODEL MONITORING REPORT\")\n",
    "    print(f\"  Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
    "    print(\"â•\" * 65)\n",
    "\n",
    "    # â”€â”€ Helper: fetch latest execution output from S3 â”€â”€â”€â”€â”€\n",
    "    def get_latest_execution_uri(schedule_name: str) -> str | None:\n",
    "        try:\n",
    "            execs = sm_client.list_monitoring_executions(\n",
    "                MonitoringScheduleName=schedule_name,\n",
    "                SortBy=\"ScheduledTime\",\n",
    "                SortOrder=\"Descending\",\n",
    "                MaxResults=1\n",
    "            )[\"MonitoringExecutionSummaries\"]\n",
    "            if not execs:\n",
    "                return None\n",
    "            return execs[0].get(\"ProcessingJobArn\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def read_s3_json(s3_uri: str) -> dict:\n",
    "        \"\"\"Read a JSON file from S3 URI.\"\"\"\n",
    "        parts  = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        bucket = parts[0]\n",
    "        key    = parts[1]\n",
    "        obj    = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        return json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "\n",
    "    def list_s3_files(prefix_uri: str) -> list:\n",
    "        parts  = prefix_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        bucket = parts[0]\n",
    "        prefix = parts[1]\n",
    "        resp   = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        return [f\"s3://{bucket}/{o['Key']}\" for o in resp.get(\"Contents\", [])]\n",
    "\n",
    "    # â”€â”€ Section 1: Data Quality Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ DATA QUALITY MONITOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    dq_output_prefix = f\"s3://{BUCKET}/{PREFIX}/monitoring/data-quality\"\n",
    "    dq_files = list_s3_files(dq_output_prefix)\n",
    "\n",
    "    violations_file  = next((f for f in dq_files if \"constraint_violations\" in f), None)\n",
    "    statistics_file  = next((f for f in dq_files if \"statistics.json\" in f), None)\n",
    "    constraints_file = next((f for f in dq_files if \"constraints.json\" in f), None)\n",
    "\n",
    "    if violations_file:\n",
    "        violations = read_s3_json(violations_file)\n",
    "        vlist      = violations.get(\"violations\", [])\n",
    "        print(f\"\\n  Total Violations : {len(vlist)}\")\n",
    "\n",
    "        if vlist:\n",
    "            print(f\"\\n  {'Feature':<35} {'Constraint':<20} {'Description'}\")\n",
    "            print(\"  \" + \"â”€\" * 75)\n",
    "            for v in vlist:\n",
    "                print(f\"  {v.get('feature_name','?'):<35} \"\n",
    "                      f\"{v.get('constraint_check_type','?'):<20} \"\n",
    "                      f\"{v.get('description','?')}\")\n",
    "        else:\n",
    "            print(\"  âœ… No data quality violations detected.\")\n",
    "    else:\n",
    "        print(\"  âš   No violation report found yet (monitor may not have run).\")\n",
    "\n",
    "    if statistics_file:\n",
    "        stats = read_s3_json(statistics_file)\n",
    "        print(f\"\\n  â”€â”€ Feature Statistics (latest execution) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        print(f\"  {'Feature':<35} {'Mean':>10} {'Std':>10} {'Missing%':>10}\")\n",
    "        print(\"  \" + \"â”€\" * 70)\n",
    "        for feat in stats.get(\"features\", []):\n",
    "            name    = feat.get(\"name\", \"?\")\n",
    "            num     = feat.get(\"numerical_statistics\", {})\n",
    "            mean    = num.get(\"mean\",   \"N/A\")\n",
    "            std     = num.get(\"std_dev\",\"N/A\")\n",
    "            missing = feat.get(\"string_statistics\", {}).get(\n",
    "                \"common_values\", [{}])[0].get(\"value\", \"N/A\")\n",
    "            pct_missing = feat.get(\"num_missing_data_points\", 0)\n",
    "            total       = feat.get(\"num_data_points\", 1)\n",
    "            miss_pct    = f\"{100 * pct_missing / total:.1f}%\" if total else \"N/A\"\n",
    "            mean_s  = f\"{mean:.4f}\" if isinstance(mean, float) else str(mean)\n",
    "            std_s   = f\"{std:.4f}\"  if isinstance(std, float)  else str(std)\n",
    "            print(f\"  {name:<35} {mean_s:>10} {std_s:>10} {miss_pct:>10}\")\n",
    "\n",
    "    # â”€â”€ Section 2: Model Quality Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ MODEL QUALITY MONITOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    mq_output_prefix = f\"s3://{BUCKET}/{PREFIX}/monitoring/model-quality\"\n",
    "    mq_files = list_s3_files(mq_output_prefix)\n",
    "\n",
    "    mq_violations = next((f for f in mq_files if \"constraint_violations\" in f), None)\n",
    "    mq_stats      = next((f for f in mq_files if \"statistics.json\" in f), None)\n",
    "\n",
    "    if mq_stats:\n",
    "        stats  = read_s3_json(mq_stats)\n",
    "        binary = stats.get(\"binary_classification_metrics\", {})\n",
    "\n",
    "        print(f\"\\n  {'Metric':<30} {'Value':>10} {'Threshold':>12} {'Status':>10}\")\n",
    "        print(\"  \" + \"â”€\" * 65)\n",
    "\n",
    "        metric_thresholds = {\n",
    "            \"accuracy\":  0.75,\n",
    "            \"auc\":       0.75,\n",
    "            \"precision\": 0.70,\n",
    "            \"recall\":    0.70,\n",
    "            \"f1\":        0.70,\n",
    "        }\n",
    "        for metric, threshold in metric_thresholds.items():\n",
    "            val    = binary.get(metric, {}).get(\"value\", None)\n",
    "            if val is None:\n",
    "                print(f\"  {metric:<30} {'N/A':>10} {threshold:>12.2f} {'?':>10}\")\n",
    "                continue\n",
    "            status = \"âœ… OK\" if val >= threshold else \"âŒ FAIL\"\n",
    "            print(f\"  {metric:<30} {val:>10.4f} {threshold:>12.2f} {status:>10}\")\n",
    "    else:\n",
    "        print(\"  âš   No model quality statistics found yet.\")\n",
    "\n",
    "    if mq_violations:\n",
    "        mv = read_s3_json(mq_violations).get(\"violations\", [])\n",
    "        print(f\"\\n  Model Quality Violations: {len(mv)}\")\n",
    "        for v in mv:\n",
    "            print(f\"    âŒ {v.get('metric_name','?')} â€” {v.get('description','?')}\")\n",
    "\n",
    "    # â”€â”€ Section 3: Endpoint Traffic Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ ENDPOINT TRAFFIC (last 1 hour via CloudWatch) â”€â”€â”€â”€\")\n",
    "    cw = boto_session.client(\"cloudwatch\")\n",
    "    now   = datetime.utcnow()\n",
    "    start = now.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "    def get_cw_metric(metric_name, stat=\"Average\"):\n",
    "        resp = cw.get_metric_statistics(\n",
    "            Namespace=\"AWS/SageMaker\",\n",
    "            MetricName=metric_name,\n",
    "            Dimensions=[\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "                {\"Name\": \"VariantName\",  \"Value\": \"AllTraffic\"},\n",
    "            ],\n",
    "            StartTime=start,\n",
    "            EndTime=now,\n",
    "            Period=3600,\n",
    "            Statistics=[stat]\n",
    "        )\n",
    "        pts = resp.get(\"Datapoints\", [])\n",
    "        return pts[0][stat] if pts else None\n",
    "\n",
    "    invocations = get_cw_metric(\"Invocations\", \"Sum\")\n",
    "    latency_avg = get_cw_metric(\"ModelLatency\", \"Average\")\n",
    "    errors_5xx  = get_cw_metric(\"Invocation5XXErrors\", \"Sum\")\n",
    "    cpu_avg     = get_cw_metric(\"CPUUtilization\", \"Average\")\n",
    "\n",
    "    print(f\"  Invocations (1h)    : {invocations or 'N/A'}\")\n",
    "    print(f\"  Avg Model Latency   : {f'{latency_avg/1000:.1f} ms' if latency_avg else 'N/A'}\")\n",
    "    print(f\"  5xx Errors          : {int(errors_5xx) if errors_5xx else 0}\")\n",
    "    print(f\"  Avg CPU Utilisation : {f'{cpu_avg:.1f}%' if cpu_avg else 'N/A'}\")\n",
    "\n",
    "    # â”€â”€ Section 4: Model Registry Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâ”€â”€ MODEL REGISTRY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    try:\n",
    "        packages = sm_client.list_model_packages(\n",
    "            ModelPackageGroupName=\"LiverDiseaseModelGroup\",\n",
    "            SortBy=\"CreationTime\",\n",
    "            SortOrder=\"Descending\",\n",
    "            MaxResults=5\n",
    "        )[\"ModelPackageSummaryList\"]\n",
    "\n",
    "        print(f\"  {'Version':<10} {'Status':<25} {'Created':<25} {'ARN'}\")\n",
    "        print(\"  \" + \"â”€\" * 90)\n",
    "        for pkg in packages:\n",
    "            ver     = pkg.get(\"ModelPackageVersion\", \"?\")\n",
    "            status  = pkg.get(\"ModelApprovalStatus\", \"?\")\n",
    "            created = pkg[\"CreationTime\"].strftime(\"%Y-%m-%d %H:%M\")\n",
    "            arn     = pkg[\"ModelPackageArn\"]\n",
    "            icon    = \"âœ…\" if status == \"Approved\" else (\"âŒ\" if status == \"Rejected\" else \"â³\")\n",
    "            print(f\"  {ver:<10} {icon} {status:<22} {created:<25} {arn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš   Could not fetch model registry: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"â•\" * 65)\n",
    "    print(\"  END OF MONITORING REPORT\")\n",
    "    print(\"â•\" * 65 + \"\\n\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 15.  EXTRACT PIPELINE ARTIFACTS  (auto-parse execution output)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def extract_pipeline_artifacts(execution_arn: str) -> dict:\n",
    "    \"\"\"\n",
    "    Automatically extracts all ARNs, S3 URIs and names produced\n",
    "    by a completed pipeline execution. No manual copy-pasting needed.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        model_data_s3       â€“ S3 URI of the XGBoost model.tar.gz\n",
    "        model_package_arn   â€“ ARN of the registered Model Package\n",
    "        model_package_name  â€“ same ARN (used as name in update_model_package)\n",
    "        training_job_name   â€“ name of the XGBoost training job\n",
    "        baseline_s3_uri     â€“ S3 URI of the monitoring baseline CSV\n",
    "        test_s3_uri         â€“ S3 URI of the test dataset\n",
    "        evaluation_results  â€“ parsed evaluation.json dict\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"â•\" * 62)\n",
    "    print(\"  EXTRACTING PIPELINE ARTIFACTS\")\n",
    "    print(\"â•\" * 62)\n",
    "\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn,\n",
    "        SortOrder=\"Ascending\"\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "\n",
    "    step_map = {s[\"StepName\"]: s for s in steps}\n",
    "    artifacts = {}\n",
    "\n",
    "    # â”€â”€ XGBoost Training Job â†’ model_data_s3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    xgb_step = step_map.get(\"TrainXGBoost\", {})\n",
    "    xgb_job_arn = xgb_step.get(\"Metadata\", {}).get(\"TrainingJob\", {}).get(\"Arn\", \"\")\n",
    "    if xgb_job_arn:\n",
    "        job_name = xgb_job_arn.split(\"/\")[-1]\n",
    "        job_desc = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        model_data_s3 = job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "        artifacts[\"training_job_name\"] = job_name\n",
    "        artifacts[\"model_data_s3\"]     = model_data_s3\n",
    "        print(f\"  Training job   : {job_name}\")\n",
    "        print(f\"  Model data S3  : {model_data_s3}\")\n",
    "    else:\n",
    "        print(\"  âš   TrainXGBoost step not found in execution.\")\n",
    "\n",
    "    # â”€â”€ Model Registry â†’ model_package_arn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    try:\n",
    "        packages = sm_client.list_model_packages(\n",
    "            ModelPackageGroupName=\"LiverDiseaseModelGroup\",\n",
    "            SortBy=\"CreationTime\",\n",
    "            SortOrder=\"Descending\",\n",
    "            MaxResults=1\n",
    "        )[\"ModelPackageSummaryList\"]\n",
    "        if packages:\n",
    "            pkg = packages[0]\n",
    "            artifacts[\"model_package_arn\"]  = pkg[\"ModelPackageArn\"]\n",
    "            artifacts[\"model_package_name\"] = pkg[\"ModelPackageArn\"]  # same value\n",
    "            artifacts[\"model_package_ver\"]  = pkg.get(\"ModelPackageVersion\", \"?\")\n",
    "            artifacts[\"approval_status\"]    = pkg.get(\"ModelApprovalStatus\", \"?\")\n",
    "            print(f\"  Model Package  : v{artifacts['model_package_ver']} \"\n",
    "                  f\"[{artifacts['approval_status']}]\")\n",
    "            print(f\"  Package ARN    : {artifacts['model_package_arn']}\")\n",
    "        else:\n",
    "            print(\"  âš   No model packages found in LiverDiseaseModelGroup.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš   Could not fetch model registry: {e}\")\n",
    "\n",
    "    # â”€â”€ Processing outputs â†’ baseline & test S3 URIs â”€â”€â”€â”€â”€\n",
    "    proc_step = step_map.get(\"LiverDiseasePreprocessing\", {})\n",
    "    proc_job_arn = proc_step.get(\"Metadata\", {}).get(\"ProcessingJob\", {}).get(\"Arn\", \"\")\n",
    "    if proc_job_arn:\n",
    "        job_name = proc_job_arn.split(\"/\")[-1]\n",
    "        proc_desc = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "        for output in proc_desc[\"ProcessingOutputConfig\"][\"Outputs\"]:\n",
    "            name = output[\"OutputName\"]\n",
    "            uri  = output[\"S3Output\"][\"S3Uri\"]\n",
    "            if name == \"baseline\":\n",
    "                artifacts[\"baseline_s3_uri\"] = uri + \"/baseline.csv\"\n",
    "                print(f\"  Baseline S3    : {artifacts['baseline_s3_uri']}\")\n",
    "            elif name == \"test\":\n",
    "                artifacts[\"test_s3_uri\"] = uri\n",
    "                print(f\"  Test data S3   : {uri}\")\n",
    "\n",
    "    # â”€â”€ Evaluation results â†’ parsed metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    eval_step = step_map.get(\"EvaluateModels\", {})\n",
    "    eval_job_arn = eval_step.get(\"Metadata\", {}).get(\"ProcessingJob\", {}).get(\"Arn\", \"\")\n",
    "    if eval_job_arn:\n",
    "        job_name = eval_job_arn.split(\"/\")[-1]\n",
    "        eval_desc = sm_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "        for output in eval_desc[\"ProcessingOutputConfig\"][\"Outputs\"]:\n",
    "            if output[\"OutputName\"] == \"evaluation\":\n",
    "                eval_uri = output[\"S3Output\"][\"S3Uri\"] + \"/evaluation.json\"\n",
    "                try:\n",
    "                    parts  = eval_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "                    obj    = s3_client.get_object(Bucket=parts[0], Key=parts[1])\n",
    "                    eval_results = json.loads(obj[\"Body\"].read())\n",
    "                    artifacts[\"evaluation_results\"] = eval_results\n",
    "                    print(f\"\\n  â”€â”€ Evaluation Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "                    for model_key in [\"xgboost\", \"random_forest\"]:\n",
    "                        m = eval_results.get(model_key, {})\n",
    "                        if \"error\" not in m:\n",
    "                            print(f\"  {model_key:<15} acc={m.get('accuracy','?')}  \"\n",
    "                                  f\"auc={m.get('auc','?')}  f1={m.get('f1','?')}\")\n",
    "                    print(f\"  Best model     : {eval_results.get('best_model','?')}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âš   Could not read evaluation.json: {e}\")\n",
    "\n",
    "    print(\"â•\" * 62)\n",
    "    return artifacts\n",
    "\n",
    "\n",
    "def approve_and_deploy(artifacts: dict,\n",
    "                       endpoint_name: str = \"liver-disease-endpoint\") -> sagemaker.predictor.Predictor:\n",
    "    \"\"\"\n",
    "    1. Auto-approves the registered model package\n",
    "    2. Deploys it to a real-time endpoint\n",
    "    Uses artifacts dict returned by extract_pipeline_artifacts().\n",
    "    \"\"\"\n",
    "    # â”€â”€ Step 1: Approve model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    pkg_arn = artifacts.get(\"model_package_arn\")\n",
    "    if not pkg_arn:\n",
    "        raise ValueError(\"model_package_arn not found in artifacts. \"\n",
    "                         \"Did the RegisterLiverDiseaseModel step succeed?\")\n",
    "\n",
    "    sm_client.update_model_package(\n",
    "        ModelPackageArn=pkg_arn,\n",
    "        ModelApprovalStatus=\"Approved\"\n",
    "    )\n",
    "    print(f\"âœ… Model approved: {pkg_arn}\")\n",
    "\n",
    "    # â”€â”€ Step 2: Deploy endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model_data_s3 = artifacts.get(\"model_data_s3\")\n",
    "    if not model_data_s3:\n",
    "        raise ValueError(\"model_data_s3 not found in artifacts.\")\n",
    "\n",
    "    predictor = deploy_model(\n",
    "        model_data_s3=model_data_s3,\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "    return predictor\n",
    "\n",
    "def cleanup(endpoint_name: str = \"liver-disease-endpoint\"):\n",
    "    \"\"\"Delete endpoint and monitoring schedules to avoid charges.\"\"\"\n",
    "    try:\n",
    "        sm_client.delete_monitoring_schedule(MonitoringScheduleName=\"liver-data-quality-monitor\")\n",
    "        sm_client.delete_monitoring_schedule(MonitoringScheduleName=\"liver-model-quality-monitor\")\n",
    "        print(\"Monitoring schedules deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Monitor delete: {e}\")\n",
    "\n",
    "    try:\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"Endpoint '{endpoint_name}' deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Endpoint delete: {e}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 17.  MAIN ORCHESTRATOR  (fully automatic end-to-end)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 62)\n",
    "    print(\"  Liver Disease ML Pipeline â€“ AWS SageMaker\")\n",
    "    print(\"=\" * 62)\n",
    "\n",
    "    # â”€â”€ STEP 0: Save all helper scripts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    save_preprocessing_script()\n",
    "    save_training_scripts()\n",
    "    setup_cicd_files()\n",
    "\n",
    "    # â”€â”€ STEP 1: Upload raw data to S3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    raw_s3 = upload_raw_data(\"Liver_Patient_Dataset__LPD__train.csv\")\n",
    "\n",
    "    # â”€â”€ STEP 2: Ingest into Feature Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = pd.read_csv(\"Liver_Patient_Dataset__LPD__train.csv\", encoding=\"latin-1\").dropna()\n",
    "    df.columns = df.columns.str.strip().str.replace(\"\\xa0\", \"\", regex=False)\n",
    "    df.rename(columns={\n",
    "        \"Age of the patient\":                   \"Age\",\n",
    "        \"Gender of the patient\":                \"Gender\",\n",
    "        \"Total Bilirubin\":                      \"Total_Bilirubin\",\n",
    "        \"Direct Bilirubin\":                     \"Direct_Bilirubin\",\n",
    "        \"Alkphos Alkaline Phosphotase\":         \"Alkaline_Phosphotase\",\n",
    "        \"Sgpt Alamine Aminotransferase\":        \"Alamine_Aminotransferase\",\n",
    "        \"Sgot Aspartate Aminotransferase\":      \"Aspartate_Aminotransferase\",\n",
    "        \"Total Protiens\":                       \"Total_Proteins\",\n",
    "        \"ALB Albumin\":                          \"Albumin\",\n",
    "        \"A/G Ratio Albumin and Globulin Ratio\": \"AG_Ratio\",\n",
    "        \"Result\":                               \"Target\"\n",
    "    }, inplace=True)\n",
    "    create_feature_group(df)\n",
    "\n",
    "    # â”€â”€ STEP 3: Build & run pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    build_pipeline()\n",
    "    execution_arn, status = monitor_pipeline_execution(\"LiverDiseasePipeline\")\n",
    "\n",
    "    if status != \"Succeeded\":\n",
    "        print(f\"\\nâŒ Pipeline failed â€” stopping. Fix the reported step and re-run.\")\n",
    "        return\n",
    "\n",
    "    # â”€â”€ STEP 4: Auto-extract all ARNs / URIs from execution â”€\n",
    "    artifacts = extract_pipeline_artifacts(execution_arn)\n",
    "\n",
    "    # â”€â”€ STEP 5: Approve model + deploy endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ENDPOINT_NAME = \"liver-disease-endpoint\"\n",
    "    try:\n",
    "        predictor = approve_and_deploy(artifacts, endpoint_name=ENDPOINT_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"âš   Deployment skipped: {e}\")\n",
    "        predictor = None\n",
    "\n",
    "    # â”€â”€ STEP 6: CloudWatch dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    create_cloudwatch_dashboard(ENDPOINT_NAME)\n",
    "\n",
    "    # â”€â”€ STEP 7: Endpoint invocation examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if predictor:\n",
    "        try:\n",
    "            invoke_endpoint_examples(ENDPOINT_NAME)\n",
    "        except Exception as e:\n",
    "            print(f\"âš   Endpoint invocation skipped: {e}\")\n",
    "\n",
    "    # â”€â”€ STEP 8: Batch transform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    test_s3_uri = artifacts.get(\"test_s3_uri\")\n",
    "    if test_s3_uri and artifacts.get(\"training_job_name\"):\n",
    "        try:\n",
    "            # â”€â”€ Strip label column from test CSV for batch transform â”€â”€\n",
    "            # The test CSV has label in col 0 + 13 features in cols 1-13.\n",
    "            # Built-in XGBoost batch transform expects features ONLY (no label).\n",
    "            import tempfile, io\n",
    "            test_key = test_s3_uri.replace(f\"s3://{BUCKET}/\", \"\") + \"/test.csv\"\n",
    "            obj       = s3_client.get_object(Bucket=BUCKET, Key=test_key)\n",
    "            test_df   = pd.read_csv(io.BytesIO(obj[\"Body\"].read()), header=None)\n",
    "            features_only = test_df.iloc[:, 1:]   # drop label column 0\n",
    "\n",
    "            # Upload features-only CSV to a separate S3 prefix\n",
    "            features_key = f\"{PREFIX}/batch-input/test_features.csv\"\n",
    "            buf = io.StringIO()\n",
    "            features_only.to_csv(buf, index=False, header=False)\n",
    "            s3_client.put_object(\n",
    "                Bucket=BUCKET, Key=features_key,\n",
    "                Body=buf.getvalue().encode(\"utf-8\")\n",
    "            )\n",
    "            batch_input_uri = f\"s3://{BUCKET}/{features_key}\"\n",
    "            print(f\"  Features-only test data â†’ {batch_input_uri}  \"\n",
    "                  f\"({len(features_only)} rows Ã— {features_only.shape[1]} cols)\")\n",
    "\n",
    "            # â”€â”€ Create named model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            model_name = \"liver-disease-xgboost\"\n",
    "            xgb_image  = sagemaker.image_uris.retrieve(\n",
    "                framework=\"xgboost\", region=region, version=\"1.5-1\"\n",
    "            )\n",
    "            try:\n",
    "                sm_client.create_model(\n",
    "                    ModelName=model_name,\n",
    "                    PrimaryContainer={\n",
    "                        \"Image\":        xgb_image,\n",
    "                        \"ModelDataUrl\": artifacts[\"model_data_s3\"],\n",
    "                    },\n",
    "                    ExecutionRoleArn=role\n",
    "                )\n",
    "                print(f\"  Model '{model_name}' created for batch transform.\")\n",
    "            except sm_client.exceptions.ResourceInUse:\n",
    "                print(f\"  Model '{model_name}' already exists â€” reusing.\")\n",
    "\n",
    "            run_batch_transform(\n",
    "                model_name=model_name,\n",
    "                test_s3_uri=batch_input_uri\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âš   Batch transform skipped: {e}\")\n",
    "    else:\n",
    "        print(\"âš   Batch transform skipped: test_s3_uri or model not available.\")\n",
    "\n",
    "    # â”€â”€ STEP 9: Setup monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    baseline_s3_uri = artifacts.get(\"baseline_s3_uri\")\n",
    "    if baseline_s3_uri and predictor:\n",
    "        try:\n",
    "            setup_model_monitoring(ENDPOINT_NAME, baseline_s3_uri)\n",
    "        except Exception as e:\n",
    "            print(f\"âš   Monitoring setup skipped: {e}\")\n",
    "    else:\n",
    "        print(\"âš   Monitoring skipped: endpoint or baseline not available.\")\n",
    "\n",
    "    # â”€â”€ STEP 10: Monitoring report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    try:\n",
    "        generate_monitoring_report(ENDPOINT_NAME)\n",
    "    except Exception as e:\n",
    "        print(f\"âš   Monitoring report skipped: {e}\")\n",
    "\n",
    "    # â”€â”€ DONE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"=\" * 62)\n",
    "    print(\"  âœ… PIPELINE COMPLETE â€” ARTIFACT SUMMARY\")\n",
    "    print(\"=\" * 62)\n",
    "    print(f\"  Endpoint        : {ENDPOINT_NAME}\")\n",
    "    print(f\"  Model data S3   : {artifacts.get('model_data_s3', 'N/A')}\")\n",
    "    print(f\"  Model Package   : {artifacts.get('model_package_arn', 'N/A')}\")\n",
    "    print(f\"  Baseline S3     : {artifacts.get('baseline_s3_uri', 'N/A')}\")\n",
    "    if artifacts.get(\"evaluation_results\"):\n",
    "        xgb = artifacts[\"evaluation_results\"].get(\"xgboost\", {})\n",
    "        print(f\"  XGBoost AUC     : {xgb.get('auc', 'N/A')}\")\n",
    "        print(f\"  XGBoost Acc     : {xgb.get('accuracy', 'N/A')}\")\n",
    "    print(\"=\" * 62)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bdc3fe-c7cf-4d06-98cb-94c5f2e0b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw score : 0.8329\n",
      "Prediction: liver_disease\n",
      "Confidence: 83.3%\n"
     ]
    }
   ],
   "source": [
    "import boto3, json\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# One patient record â€” 13 features (NO label column):\n",
    "# Age, Gender, TotalBili, DirectBili, AlkPhos, SGPT, SGOT, TotProt, Albumin, AGRatio, BiliRatio, EnzRatio, AgeGrp\n",
    "patient = \"65,1,0.7,0.1,187,16,18,6.8,3.3,0.90,0.14,0.89,2\"\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=\"liver-disease-endpoint\",\n",
    "    ContentType=\"text/csv\",\n",
    "    Body=patient\n",
    ")\n",
    "\n",
    "score = float(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(f\"Raw score : {score:.4f}\")\n",
    "print(f\"Prediction: {'liver_disease' if score >= 0.5 else 'no_disease'}\")\n",
    "print(f\"Confidence: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835eb49-c8b2-4b0f-8435-f7369c3d0545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
